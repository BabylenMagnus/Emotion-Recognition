{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "BS = 50\n",
    "num_classes = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'/media/evgeniy/Seagate Expansion Drive/код/EmotionRecognition'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train': datasets.ImageFolder('image', transform=transform['train']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = len(data['train'])\n",
    "test_data_size = len(data['test'])\n",
    "valid_data_size = len(data['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(data['train'], batch_size=BS, shuffle=True, \n",
    "                        num_workers=6)\n",
    "test_data = DataLoader(data['test'], batch_size=BS, shuffle=True, \n",
    "                       num_workers=6)\n",
    "valid_data = DataLoader(data['valid'], batch_size=BS, shuffle=True, \n",
    "                        num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet34 = models.resnet34(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet34' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-393a1f43ba84>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mresnet34\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresnet34\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'resnet34' is not defined"
     ]
    }
   ],
   "source": [
    "resnet101.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.4),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "resnet34 = resnet34.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.SGD(resnet101.parameters(), lr=1e-3,\n",
    "                      nesterov=True, momentum=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101 = models.resnet101(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.4),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = torch.load('resnet34_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34.class_to_idx = data['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=7, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "Batch number: 000, Training: Loss: 0.2551, Accuracy: 0.9000\n",
      "Batch number: 001, Training: Loss: 0.1828, Accuracy: 0.9600\n",
      "Batch number: 002, Training: Loss: 0.3498, Accuracy: 0.9000\n",
      "Batch number: 003, Training: Loss: 0.1808, Accuracy: 0.9200\n",
      "Batch number: 004, Training: Loss: 0.1994, Accuracy: 0.9400\n",
      "Batch number: 005, Training: Loss: 0.2884, Accuracy: 0.9200\n",
      "Batch number: 006, Training: Loss: 0.2789, Accuracy: 0.9200\n",
      "Batch number: 007, Training: Loss: 0.2868, Accuracy: 0.9200\n",
      "Batch number: 008, Training: Loss: 0.4001, Accuracy: 0.8800\n",
      "Batch number: 009, Training: Loss: 0.2451, Accuracy: 0.9400\n",
      "Batch number: 010, Training: Loss: 0.2043, Accuracy: 0.9800\n",
      "Batch number: 011, Training: Loss: 0.3074, Accuracy: 0.9200\n",
      "Batch number: 012, Training: Loss: 0.2865, Accuracy: 0.9400\n",
      "Batch number: 013, Training: Loss: 0.3665, Accuracy: 0.9000\n",
      "Batch number: 014, Training: Loss: 0.2364, Accuracy: 0.9000\n",
      "Batch number: 015, Training: Loss: 0.2345, Accuracy: 0.9800\n",
      "Batch number: 016, Training: Loss: 0.2578, Accuracy: 0.9200\n",
      "Batch number: 017, Training: Loss: 0.2184, Accuracy: 0.9200\n",
      "Batch number: 018, Training: Loss: 0.2945, Accuracy: 0.9400\n",
      "Batch number: 019, Training: Loss: 0.3823, Accuracy: 0.8400\n",
      "Batch number: 020, Training: Loss: 0.2719, Accuracy: 0.9400\n",
      "Batch number: 021, Training: Loss: 0.1904, Accuracy: 0.9600\n",
      "Batch number: 022, Training: Loss: 0.2825, Accuracy: 0.9400\n",
      "Batch number: 023, Training: Loss: 0.2250, Accuracy: 0.9600\n",
      "Batch number: 024, Training: Loss: 0.2438, Accuracy: 0.9200\n",
      "Batch number: 025, Training: Loss: 0.3561, Accuracy: 0.9400\n",
      "Batch number: 026, Training: Loss: 0.1825, Accuracy: 0.9600\n",
      "Batch number: 027, Training: Loss: 0.2416, Accuracy: 0.9000\n",
      "Batch number: 028, Training: Loss: 0.2209, Accuracy: 0.9400\n",
      "Batch number: 029, Training: Loss: 0.3011, Accuracy: 0.9400\n",
      "Batch number: 030, Training: Loss: 0.1968, Accuracy: 0.9600\n",
      "Batch number: 031, Training: Loss: 0.1907, Accuracy: 0.9600\n",
      "Batch number: 032, Training: Loss: 0.3334, Accuracy: 0.8800\n",
      "Batch number: 033, Training: Loss: 0.2005, Accuracy: 0.9600\n",
      "Batch number: 034, Training: Loss: 0.3287, Accuracy: 0.9000\n",
      "Batch number: 035, Training: Loss: 0.3815, Accuracy: 0.8400\n",
      "Batch number: 036, Training: Loss: 0.2244, Accuracy: 0.9600\n",
      "Batch number: 037, Training: Loss: 0.3285, Accuracy: 0.9400\n",
      "Batch number: 038, Training: Loss: 0.3394, Accuracy: 0.8333\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, 1))\n",
    "\n",
    "    # Set to training mode\n",
    "    resnet34.train()\n",
    "#     his.append(acc.item())\n",
    "    # Loss and Accuracy within the epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = resnet34(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "        print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet34, 'resnet34_fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 000, Validation: Loss: 1.7158, Accuracy: 0.3600\n",
      "Validation Batch number: 001, Validation: Loss: 1.7020, Accuracy: 0.3000\n",
      "Epoch : 009, Training: Loss: 1.1545, Accuracy: 58.8389%, \n",
      "\t\tValidation : Loss : 5.1355, Accuracy: 102.8571%,\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    " \n",
    "    # Set to evaluation mode\n",
    "    resnet34.eval()\n",
    " \n",
    "    # Validation loop\n",
    "    for j, (inputs, labels) in enumerate(valid_data):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    " \n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = resnet34(inputs)\n",
    " \n",
    "        # Compute loss\n",
    "        loss = loss_func(outputs, labels)\n",
    " \n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        valid_loss += loss.item() * inputs.size(0)\n",
    " \n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    " \n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    " \n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        valid_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "        print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "     \n",
    "# Find average training loss and training accuracy\n",
    "avg_train_loss = train_loss/train_data_size\n",
    "avg_train_acc = train_acc/float(train_data_size)\n",
    " \n",
    "# Find average training loss and training accuracy\n",
    "avg_valid_loss = valid_loss/valid_data_size\n",
    "avg_valid_acc = valid_acc/float(valid_data_size)\n",
    " \n",
    "history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "         \n",
    " \n",
    "print(\"Epoch : {:03d}, Training: Loss: {:.4f}, \\\n",
    "Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%,\\\n",
    "\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, \n",
    "                      avg_valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(resnet34, optimizer,\n",
    "                                    loss_func, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(resnet34, device=device, metrics={\n",
    "    'accuracy': Accuracy(),\n",
    "    'nll': Loss(loss_func)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(trainer):\n",
    "    print(f\"Epoch[{trainer.state.epoch}] Loss: {trainer.state.output}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_data)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch: {trainer.state.epoch}  \\\n",
    "    Avg accuracy: {metrics['accuracy']} Avg loss: {metrics['nll']}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(valid_data)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch} \\\n",
    "     Avg accuracy: {metrics['accuracy']} Avg loss: {metrics['nll']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Loss: 1.0817815065383911\n",
      "Epoch[1] Loss: 1.2974778413772583\n",
      "Epoch[1] Loss: 1.0173799991607666\n",
      "Epoch[1] Loss: 1.3856123685836792\n",
      "Epoch[1] Loss: 1.1279902458190918\n",
      "Epoch[1] Loss: 1.3473247289657593\n",
      "Epoch[1] Loss: 0.9862923622131348\n",
      "Epoch[1] Loss: 1.0291169881820679\n",
      "Epoch[1] Loss: 1.3244439363479614\n",
      "Epoch[1] Loss: 1.1804990768432617\n",
      "Epoch[1] Loss: 1.3402515649795532\n",
      "Epoch[1] Loss: 1.1243102550506592\n",
      "Epoch[1] Loss: 1.182692527770996\n",
      "Epoch[1] Loss: 1.2603952884674072\n",
      "Epoch[1] Loss: 1.2927770614624023\n",
      "Epoch[1] Loss: 1.2987306118011475\n",
      "Epoch[1] Loss: 1.124316692352295\n",
      "Epoch[1] Loss: 1.0488361120224\n",
      "Epoch[1] Loss: 0.8671000599861145\n",
      "Epoch[1] Loss: 1.1770747900009155\n",
      "Epoch[1] Loss: 1.067082166671753\n",
      "Epoch[1] Loss: 1.21915864944458\n",
      "Epoch[1] Loss: 0.990007758140564\n",
      "Epoch[1] Loss: 1.2378064393997192\n",
      "Epoch[1] Loss: 0.9518067836761475\n",
      "Epoch[1] Loss: 1.077283501625061\n",
      "Epoch[1] Loss: 1.1019188165664673\n",
      "Epoch[1] Loss: 1.1752203702926636\n",
      "Epoch[1] Loss: 1.340378761291504\n",
      "Epoch[1] Loss: 1.0322721004486084\n",
      "Epoch[1] Loss: 1.2286193370819092\n",
      "Epoch[1] Loss: 1.0640785694122314\n",
      "Epoch[1] Loss: 1.0744237899780273\n",
      "Epoch[1] Loss: 1.2083477973937988\n",
      "Epoch[1] Loss: 1.0311720371246338\n",
      "Epoch[1] Loss: 1.0935848951339722\n",
      "Epoch[1] Loss: 1.2561167478561401\n",
      "Epoch[1] Loss: 1.0662840604782104\n",
      "Epoch[1] Loss: 1.2993615865707397\n",
      "Training Results - Epoch: 1      Avg accuracy: 0.6323221757322176 Avg loss: 1.0624106445945956\n",
      "Validation Results - Epoch: 1      Avg accuracy: 0.34285714285714286 Avg loss: 1.6882938657488142\n",
      "Epoch[2] Loss: 1.235426425933838\n",
      "Epoch[2] Loss: 1.1942453384399414\n",
      "Epoch[2] Loss: 1.2575066089630127\n",
      "Epoch[2] Loss: 1.1675680875778198\n",
      "Epoch[2] Loss: 1.0595916509628296\n",
      "Epoch[2] Loss: 1.1548757553100586\n",
      "Epoch[2] Loss: 0.9598484039306641\n",
      "Epoch[2] Loss: 1.0505578517913818\n",
      "Epoch[2] Loss: 1.122533917427063\n",
      "Epoch[2] Loss: 1.1908196210861206\n",
      "Epoch[2] Loss: 1.2069607973098755\n",
      "Epoch[2] Loss: 1.3170074224472046\n",
      "Epoch[2] Loss: 1.0850319862365723\n",
      "Epoch[2] Loss: 1.0259385108947754\n",
      "Epoch[2] Loss: 1.021754264831543\n",
      "Epoch[2] Loss: 1.0399428606033325\n",
      "Epoch[2] Loss: 1.1509485244750977\n",
      "Epoch[2] Loss: 1.0262640714645386\n",
      "Epoch[2] Loss: 0.9946302771568298\n",
      "Epoch[2] Loss: 1.104070782661438\n",
      "Epoch[2] Loss: 1.1210038661956787\n",
      "Epoch[2] Loss: 1.090287208557129\n",
      "Epoch[2] Loss: 1.0719952583312988\n",
      "Epoch[2] Loss: 1.2542668581008911\n",
      "Epoch[2] Loss: 1.122596025466919\n",
      "Epoch[2] Loss: 1.1587566137313843\n",
      "Epoch[2] Loss: 0.881756067276001\n",
      "Epoch[2] Loss: 1.0776162147521973\n",
      "Epoch[2] Loss: 1.2993252277374268\n",
      "Epoch[2] Loss: 1.0192874670028687\n",
      "Epoch[2] Loss: 1.2656352519989014\n",
      "Epoch[2] Loss: 1.1740094423294067\n",
      "Epoch[2] Loss: 1.130103349685669\n",
      "Epoch[2] Loss: 1.3610153198242188\n",
      "Epoch[2] Loss: 1.0324138402938843\n",
      "Epoch[2] Loss: 1.1051901578903198\n",
      "Epoch[2] Loss: 1.0733147859573364\n",
      "Epoch[2] Loss: 1.0998966693878174\n",
      "Epoch[2] Loss: 0.9776474833488464\n",
      "Training Results - Epoch: 2      Avg accuracy: 0.6385983263598326 Avg loss: 1.036355527125642\n",
      "Validation Results - Epoch: 2      Avg accuracy: 0.35714285714285715 Avg loss: 1.6834230082375663\n",
      "Epoch[3] Loss: 1.1393299102783203\n",
      "Epoch[3] Loss: 1.1577683687210083\n",
      "Epoch[3] Loss: 1.1344255208969116\n",
      "Epoch[3] Loss: 1.0347980260849\n",
      "Epoch[3] Loss: 1.0078409910202026\n",
      "Epoch[3] Loss: 1.018446445465088\n",
      "Epoch[3] Loss: 1.1794791221618652\n",
      "Epoch[3] Loss: 1.1438629627227783\n",
      "Epoch[3] Loss: 1.2879133224487305\n",
      "Epoch[3] Loss: 1.0457611083984375\n",
      "Epoch[3] Loss: 1.131154179573059\n",
      "Epoch[3] Loss: 0.9923309087753296\n",
      "Epoch[3] Loss: 1.1785012483596802\n",
      "Epoch[3] Loss: 0.9426909685134888\n",
      "Epoch[3] Loss: 1.0267252922058105\n",
      "Epoch[3] Loss: 1.344054102897644\n",
      "Epoch[3] Loss: 1.0831960439682007\n",
      "Epoch[3] Loss: 0.7710331082344055\n",
      "Epoch[3] Loss: 1.1367911100387573\n",
      "Epoch[3] Loss: 1.1153137683868408\n",
      "Epoch[3] Loss: 1.064796805381775\n",
      "Epoch[3] Loss: 0.9690910577774048\n",
      "Epoch[3] Loss: 1.1043262481689453\n",
      "Epoch[3] Loss: 1.3884109258651733\n",
      "Epoch[3] Loss: 1.0496703386306763\n",
      "Epoch[3] Loss: 1.3620593547821045\n",
      "Epoch[3] Loss: 0.8476976752281189\n",
      "Epoch[3] Loss: 1.2175613641738892\n",
      "Epoch[3] Loss: 1.0268985033035278\n",
      "Epoch[3] Loss: 1.058200478553772\n",
      "Epoch[3] Loss: 1.0680992603302002\n",
      "Epoch[3] Loss: 1.3118066787719727\n",
      "Epoch[3] Loss: 1.1352571249008179\n",
      "Epoch[3] Loss: 1.1590362787246704\n",
      "Epoch[3] Loss: 1.0982102155685425\n",
      "Epoch[3] Loss: 1.0133428573608398\n",
      "Epoch[3] Loss: 1.0877704620361328\n",
      "Epoch[3] Loss: 1.1914982795715332\n",
      "Epoch[3] Loss: 1.6628001928329468\n",
      "Training Results - Epoch: 3      Avg accuracy: 0.6469665271966527 Avg loss: 1.0139354070858975\n",
      "Validation Results - Epoch: 3      Avg accuracy: 0.35714285714285715 Avg loss: 1.6790986401694161\n",
      "Epoch[4] Loss: 1.3314076662063599\n",
      "Epoch[4] Loss: 1.0457565784454346\n",
      "Epoch[4] Loss: 0.9997097253799438\n",
      "Epoch[4] Loss: 0.974588930606842\n",
      "Epoch[4] Loss: 0.9986988306045532\n",
      "Epoch[4] Loss: 1.123383641242981\n",
      "Epoch[4] Loss: 1.250095248222351\n",
      "Epoch[4] Loss: 1.2329065799713135\n",
      "Epoch[4] Loss: 1.0081919431686401\n",
      "Epoch[4] Loss: 1.0556464195251465\n",
      "Epoch[4] Loss: 1.2717738151550293\n",
      "Epoch[4] Loss: 1.0901389122009277\n",
      "Epoch[4] Loss: 1.1249194145202637\n",
      "Epoch[4] Loss: 1.3145561218261719\n",
      "Epoch[4] Loss: 1.2221983671188354\n",
      "Epoch[4] Loss: 0.9734412431716919\n",
      "Epoch[4] Loss: 1.0495418310165405\n",
      "Epoch[4] Loss: 1.0174341201782227\n",
      "Epoch[4] Loss: 0.9490276575088501\n",
      "Epoch[4] Loss: 1.1294512748718262\n",
      "Epoch[4] Loss: 0.9620527625083923\n",
      "Epoch[4] Loss: 1.2053346633911133\n",
      "Epoch[4] Loss: 1.1594624519348145\n",
      "Epoch[4] Loss: 1.0358424186706543\n",
      "Epoch[4] Loss: 1.038486123085022\n",
      "Epoch[4] Loss: 1.1930736303329468\n",
      "Epoch[4] Loss: 1.1301145553588867\n",
      "Epoch[4] Loss: 1.111311435699463\n",
      "Epoch[4] Loss: 0.9419719576835632\n",
      "Epoch[4] Loss: 1.122658133506775\n",
      "Epoch[4] Loss: 0.8151236772537231\n",
      "Epoch[4] Loss: 1.033748984336853\n",
      "Epoch[4] Loss: 1.0841602087020874\n",
      "Epoch[4] Loss: 1.1098222732543945\n",
      "Epoch[4] Loss: 1.139758586883545\n",
      "Epoch[4] Loss: 1.1310170888900757\n",
      "Epoch[4] Loss: 1.0238898992538452\n",
      "Epoch[4] Loss: 1.2613952159881592\n",
      "Epoch[4] Loss: 0.8723892569541931\n",
      "Training Results - Epoch: 4      Avg accuracy: 0.649581589958159 Avg loss: 0.9891092742960822\n",
      "Validation Results - Epoch: 4      Avg accuracy: 0.35714285714285715 Avg loss: 1.66291310106005\n",
      "Epoch[5] Loss: 0.9632670879364014\n",
      "Epoch[5] Loss: 1.0632957220077515\n",
      "Epoch[5] Loss: 1.0689036846160889\n",
      "Epoch[5] Loss: 1.0461217164993286\n",
      "Epoch[5] Loss: 1.1330331563949585\n",
      "Epoch[5] Loss: 1.0461244583129883\n",
      "Epoch[5] Loss: 0.950596034526825\n",
      "Epoch[5] Loss: 1.0555402040481567\n",
      "Epoch[5] Loss: 1.284479022026062\n",
      "Epoch[5] Loss: 0.974367618560791\n",
      "Epoch[5] Loss: 1.1060785055160522\n",
      "Epoch[5] Loss: 1.1417404413223267\n",
      "Epoch[5] Loss: 1.0193593502044678\n",
      "Epoch[5] Loss: 0.9425087571144104\n",
      "Epoch[5] Loss: 1.0517284870147705\n",
      "Epoch[5] Loss: 1.3789992332458496\n",
      "Epoch[5] Loss: 1.094282627105713\n",
      "Epoch[5] Loss: 1.1222648620605469\n",
      "Epoch[5] Loss: 1.1640424728393555\n",
      "Epoch[5] Loss: 0.9925577640533447\n",
      "Epoch[5] Loss: 1.0508766174316406\n",
      "Epoch[5] Loss: 1.0999678373336792\n",
      "Epoch[5] Loss: 1.0111218690872192\n",
      "Epoch[5] Loss: 0.9931293725967407\n",
      "Epoch[5] Loss: 0.8481263518333435\n",
      "Epoch[5] Loss: 1.0408531427383423\n",
      "Epoch[5] Loss: 1.2969353199005127\n",
      "Epoch[5] Loss: 0.9960035085678101\n",
      "Epoch[5] Loss: 1.043021559715271\n",
      "Epoch[5] Loss: 1.1905120611190796\n",
      "Epoch[5] Loss: 1.0713844299316406\n",
      "Epoch[5] Loss: 1.0485700368881226\n",
      "Epoch[5] Loss: 0.9661089181900024\n",
      "Epoch[5] Loss: 0.9997029900550842\n",
      "Epoch[5] Loss: 1.1011431217193604\n",
      "Epoch[5] Loss: 1.1236613988876343\n",
      "Epoch[5] Loss: 1.0293028354644775\n",
      "Epoch[5] Loss: 1.1251060962677002\n",
      "Epoch[5] Loss: 1.356940746307373\n",
      "Training Results - Epoch: 5      Avg accuracy: 0.6542887029288703 Avg loss: 0.9756164755906022\n",
      "Validation Results - Epoch: 5      Avg accuracy: 0.37142857142857144 Avg loss: 1.6499496868678503\n",
      "Wall time: 3min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 195\n",
       "\tepoch: 5\n",
       "\tepoch_length: 39\n",
       "\tmax_epochs: 5\n",
       "\toutput: 1.356940746307373\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: 12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.run(train_data, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'resnet101': resnet34.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    },\n",
    "    f'{PATH}/resnet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[43] Loss: 0.003811061382293701\n",
      "Epoch[43] Loss: 0.18583130836486816\n",
      "Epoch[43] Loss: 0.007884114980697632\n",
      "Epoch[43] Loss: 0.011762768030166626\n",
      "Epoch[43] Loss: 0.007486745715141296\n",
      "Epoch[43] Loss: 0.023989051580429077\n",
      "Epoch[43] Loss: 0.009007930755615234\n",
      "Epoch[43] Loss: 0.045586779713630676\n",
      "Epoch[43] Loss: 0.005712032318115234\n",
      "Epoch[43] Loss: 0.007774919271469116\n",
      "Epoch[43] Loss: 0.11189024150371552\n",
      "Epoch[43] Loss: 0.019476711750030518\n",
      "Epoch[43] Loss: 0.0076589435338974\n",
      "Epoch[43] Loss: 0.004034385085105896\n",
      "Epoch[43] Loss: 0.004237189888954163\n",
      "Epoch[43] Loss: 0.0041923075914382935\n",
      "Epoch[43] Loss: 0.0027389079332351685\n",
      "Epoch[43] Loss: 0.018731504678726196\n",
      "Epoch[43] Loss: 0.09035556018352509\n",
      "Epoch[43] Loss: 0.05137135833501816\n",
      "Epoch[43] Loss: 0.007749393582344055\n",
      "Epoch[43] Loss: 0.01083354651927948\n",
      "Epoch[43] Loss: 0.0016209036111831665\n",
      "Epoch[43] Loss: 0.010196298360824585\n",
      "Epoch[43] Loss: 0.003902226686477661\n",
      "Epoch[43] Loss: 0.07697638124227524\n",
      "Epoch[43] Loss: 0.003788858652114868\n",
      "Epoch[43] Loss: 0.013343513011932373\n",
      "Epoch[43] Loss: 0.008828602731227875\n",
      "Epoch[43] Loss: 0.021684706211090088\n",
      "Epoch[43] Loss: 0.0014666914939880371\n",
      "Epoch[43] Loss: 0.002123609185218811\n",
      "Epoch[43] Loss: 0.0018225312232971191\n",
      "Epoch[43] Loss: 0.006148509681224823\n",
      "Epoch[43] Loss: 0.031207039952278137\n",
      "Epoch[43] Loss: 0.01964259147644043\n",
      "Epoch[43] Loss: 0.03863421827554703\n",
      "Epoch[43] Loss: 0.004181474447250366\n",
      "Epoch[43] Loss: 0.00962790846824646\n",
      "Epoch[43] Loss: 0.004479661583900452\n",
      "Epoch[43] Loss: 0.0067418962717056274\n",
      "Epoch[43] Loss: 0.11043170094490051\n",
      "Epoch[43] Loss: 0.02689497172832489\n",
      "Epoch[43] Loss: 0.003835245966911316\n",
      "Epoch[43] Loss: 0.002891629934310913\n",
      "Epoch[43] Loss: 0.0012740790843963623\n",
      "Epoch[43] Loss: 0.0028670281171798706\n",
      "Epoch[43] Loss: 0.002800002694129944\n",
      "Epoch[43] Loss: 0.025653183460235596\n",
      "Epoch[43] Loss: 0.006150722503662109\n",
      "Epoch[43] Loss: 0.0015869885683059692\n",
      "Epoch[43] Loss: 0.0033702850341796875\n",
      "Epoch[43] Loss: 0.09195457398891449\n",
      "Epoch[43] Loss: 0.004915341734886169\n",
      "Epoch[43] Loss: 0.02942056581377983\n",
      "Training Results - Epoch: 43      Avg accuracy: 0.9971334971334971 Avg loss: 0.007367579054192661\n",
      "Validation Results - Epoch: 43      Avg accuracy: 0.6 Avg loss: 2.039862060546875\n",
      "Epoch[44] Loss: 0.004607245326042175\n",
      "Epoch[44] Loss: 0.04824240505695343\n",
      "Epoch[44] Loss: 0.0007398277521133423\n",
      "Epoch[44] Loss: 0.004186868667602539\n",
      "Epoch[44] Loss: 0.055753543972969055\n",
      "Epoch[44] Loss: 0.001823514699935913\n",
      "Epoch[44] Loss: 0.002974182367324829\n",
      "Epoch[44] Loss: 0.02019333839416504\n",
      "Epoch[44] Loss: 0.06507295370101929\n",
      "Epoch[44] Loss: 0.001791641116142273\n",
      "Epoch[44] Loss: 0.03929610550403595\n",
      "Epoch[44] Loss: 0.0019494444131851196\n",
      "Epoch[44] Loss: 0.02747323364019394\n",
      "Epoch[44] Loss: 0.006556957960128784\n",
      "Epoch[44] Loss: 0.002213984727859497\n",
      "Epoch[44] Loss: 0.003664076328277588\n",
      "Epoch[44] Loss: 0.05270305275917053\n",
      "Epoch[44] Loss: 0.051138073205947876\n",
      "Epoch[44] Loss: 0.016850754618644714\n",
      "Epoch[44] Loss: 0.038725484162569046\n",
      "Epoch[44] Loss: 0.048212915658950806\n",
      "Epoch[44] Loss: 0.05978696793317795\n",
      "Epoch[44] Loss: 0.1531877964735031\n",
      "Epoch[44] Loss: 0.0039027035236358643\n",
      "Epoch[44] Loss: 0.05955803394317627\n",
      "Epoch[44] Loss: 0.01101277768611908\n",
      "Epoch[44] Loss: 0.11028705537319183\n",
      "Epoch[44] Loss: 0.002918437123298645\n",
      "Epoch[44] Loss: 0.04453378915786743\n",
      "Epoch[44] Loss: 0.003482729196548462\n",
      "Epoch[44] Loss: 0.026001274585723877\n",
      "Epoch[44] Loss: 0.12175565958023071\n",
      "Epoch[44] Loss: 0.002870693802833557\n",
      "Epoch[44] Loss: 0.0023929476737976074\n",
      "Epoch[44] Loss: 0.0097237229347229\n",
      "Epoch[44] Loss: 0.0028118491172790527\n",
      "Epoch[44] Loss: 0.020578697323799133\n",
      "Epoch[44] Loss: 0.006536990404129028\n",
      "Epoch[44] Loss: 0.015151679515838623\n",
      "Epoch[44] Loss: 0.00425313413143158\n",
      "Epoch[44] Loss: 0.180029958486557\n",
      "Epoch[44] Loss: 0.004266723990440369\n",
      "Epoch[44] Loss: 0.0023298263549804688\n",
      "Epoch[44] Loss: 0.008002161979675293\n",
      "Epoch[44] Loss: 0.002989470958709717\n",
      "Epoch[44] Loss: 0.003688812255859375\n",
      "Epoch[44] Loss: 0.01580694317817688\n",
      "Epoch[44] Loss: 0.013686731457710266\n",
      "Epoch[44] Loss: 0.02666105329990387\n",
      "Epoch[44] Loss: 0.005125716328620911\n",
      "Epoch[44] Loss: 0.003942713141441345\n",
      "Epoch[44] Loss: 0.012559756636619568\n",
      "Epoch[44] Loss: 0.08484436571598053\n",
      "Epoch[44] Loss: 0.0104704350233078\n",
      "Epoch[44] Loss: 0.10414598137140274\n",
      "Epoch[44] Loss: 0.006820753216743469\n",
      "Epoch[44] Loss: 0.052129603922367096\n",
      "Epoch[44] Loss: 0.006778046488761902\n",
      "Epoch[44] Loss: 0.020017415285110474\n",
      "Epoch[44] Loss: 0.02483333647251129\n",
      "Epoch[44] Loss: 0.0041067153215408325\n",
      "Epoch[44] Loss: 0.014953061938285828\n",
      "Epoch[44] Loss: 0.003403604030609131\n",
      "Epoch[44] Loss: 0.0036508888006210327\n",
      "Epoch[44] Loss: 0.00334283709526062\n",
      "Epoch[44] Loss: 0.0404060035943985\n",
      "Epoch[44] Loss: 0.02154242992401123\n",
      "Epoch[44] Loss: 0.0272158682346344\n",
      "Epoch[44] Loss: 0.008426651358604431\n",
      "Epoch[44] Loss: 0.016557514667510986\n",
      "Epoch[44] Loss: 0.005345791578292847\n",
      "Epoch[44] Loss: 0.014985017478466034\n",
      "Epoch[44] Loss: 0.008218809962272644\n",
      "Epoch[44] Loss: 0.05986270308494568\n",
      "Epoch[44] Loss: 0.03539005666971207\n",
      "Epoch[44] Loss: 0.005966290831565857\n",
      "Epoch[44] Loss: 0.10849447548389435\n",
      "Training Results - Epoch: 44      Avg accuracy: 0.9954954954954955 Avg loss: 0.011812254945144216\n",
      "Validation Results - Epoch: 44      Avg accuracy: 0.5571428571428572 Avg loss: 2.2042060170854842\n",
      "Epoch[45] Loss: 0.003939718008041382\n",
      "Epoch[45] Loss: 0.06436052918434143\n",
      "Epoch[45] Loss: 0.05135565996170044\n",
      "Epoch[45] Loss: 0.008422240614891052\n",
      "Epoch[45] Loss: 0.00623612105846405\n",
      "Epoch[45] Loss: 0.04280775785446167\n",
      "Epoch[45] Loss: 0.0026896893978118896\n",
      "Epoch[45] Loss: 0.01771862804889679\n",
      "Epoch[45] Loss: 0.006757333874702454\n",
      "Epoch[45] Loss: 0.043713927268981934\n",
      "Epoch[45] Loss: 0.004940912127494812\n",
      "Epoch[45] Loss: 0.0038656890392303467\n",
      "Epoch[45] Loss: 0.0024002641439437866\n",
      "Epoch[45] Loss: 0.007332637906074524\n",
      "Epoch[45] Loss: 0.016074001789093018\n",
      "Epoch[45] Loss: 0.008842632174491882\n",
      "Epoch[45] Loss: 0.0049204230308532715\n",
      "Epoch[45] Loss: 0.012531593441963196\n",
      "Epoch[45] Loss: 0.01887771487236023\n",
      "Epoch[45] Loss: 0.20107319951057434\n",
      "Epoch[45] Loss: 0.044013798236846924\n",
      "Epoch[45] Loss: 0.006143435835838318\n",
      "Epoch[45] Loss: 0.028932154178619385\n",
      "Epoch[45] Loss: 0.0023389607667922974\n",
      "Epoch[45] Loss: 0.0198819637298584\n",
      "Epoch[45] Loss: 0.025908268988132477\n",
      "Epoch[45] Loss: 0.037129297852516174\n",
      "Epoch[45] Loss: 0.004315122961997986\n",
      "Epoch[45] Loss: 0.034227222204208374\n",
      "Epoch[45] Loss: 0.057900846004486084\n",
      "Epoch[45] Loss: 0.009483307600021362\n",
      "Epoch[45] Loss: 0.14241348206996918\n",
      "Epoch[45] Loss: 0.008206933736801147\n",
      "Epoch[45] Loss: 0.002590194344520569\n",
      "Epoch[45] Loss: 0.0030448585748672485\n",
      "Epoch[45] Loss: 0.0017676353454589844\n",
      "Epoch[45] Loss: 0.0029550790786743164\n",
      "Epoch[45] Loss: 0.003837078809738159\n",
      "Epoch[45] Loss: 0.0015983134508132935\n",
      "Epoch[45] Loss: 0.003661230206489563\n",
      "Epoch[45] Loss: 0.0074494630098342896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: .\n",
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<timed eval>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, data, max_epochs, epoch_length, seed)\u001B[0m\n\u001B[0;32m    848\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    849\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 850\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_internal_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    851\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    852\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_setup_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    950\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_iter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_len\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    951\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Engine run is terminating due to exception: %s.\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 952\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    954\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_iter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_len\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_handle_exception\u001B[1;34m(self, e)\u001B[0m\n\u001B[0;32m    714\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEXCEPTION_RAISED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    715\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 716\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    717\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    935\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_setup_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    936\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 937\u001B[1;33m                 \u001B[0mhours\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmins\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_run_once_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    938\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    939\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhours\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmins\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_run_once_on_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    703\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Current run is terminating due to exception: %s.\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 705\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    706\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    707\u001B[0m         \u001B[0mtime_taken\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_handle_exception\u001B[1;34m(self, e)\u001B[0m\n\u001B[0;32m    714\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEXCEPTION_RAISED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    715\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 716\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    717\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_run_once_on_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    686\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miteration\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    687\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mITERATION_STARTED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 688\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_process_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    689\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mITERATION_COMPLETED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    690\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\__init__.py\u001B[0m in \u001B[0;36m_update\u001B[1;34m(engine, batch)\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0moutput_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mEngine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_update\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\ignite\\engine\\__init__.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x, y, y_pred, loss)\u001B[0m\n\u001B[0;32m     17\u001B[0m                               \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m                               \u001B[0mprepare_batch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_prepare_batch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m                               output_transform=lambda x, y, y_pred, loss: loss.item()):\n\u001B[0m\u001B[0;32m     20\u001B[0m     \"\"\"\n\u001B[0;32m     21\u001B[0m     \u001B[0mFactory\u001B[0m \u001B[0mfunction\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcreating\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtrainer\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msupervised\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.run(train_data, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet34,\n",
    "    f'{PATH}/resnet34_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7] Loss: 0.6600565314292908\n",
      "Epoch[7] Loss: 0.05696873366832733\n",
      "Epoch[7] Loss: 0.3519812226295471\n",
      "Epoch[7] Loss: 0.4575202763080597\n",
      "Epoch[7] Loss: 0.24591875076293945\n",
      "Epoch[7] Loss: 0.23707084357738495\n",
      "Epoch[7] Loss: 0.46361106634140015\n",
      "Epoch[7] Loss: 0.402351975440979\n",
      "Epoch[7] Loss: 0.2874239981174469\n",
      "Epoch[7] Loss: 0.34599876403808594\n",
      "Epoch[7] Loss: 0.7171586751937866\n",
      "Epoch[7] Loss: 0.28149551153182983\n",
      "Epoch[7] Loss: 0.45462095737457275\n",
      "Epoch[7] Loss: 0.29328253865242004\n",
      "Epoch[7] Loss: 0.27080458402633667\n",
      "Epoch[7] Loss: 0.09270523488521576\n",
      "Epoch[7] Loss: 0.400163471698761\n",
      "Epoch[7] Loss: 0.2696250081062317\n",
      "Epoch[7] Loss: 0.25013473629951477\n",
      "Epoch[7] Loss: 0.2283349633216858\n",
      "Epoch[7] Loss: 0.31190451979637146\n",
      "Epoch[7] Loss: 0.3916991651058197\n",
      "Epoch[7] Loss: 0.3957797586917877\n",
      "Epoch[7] Loss: 0.2511162757873535\n",
      "Epoch[7] Loss: 0.2201136350631714\n",
      "Epoch[7] Loss: 0.23781412839889526\n",
      "Epoch[7] Loss: 0.30612391233444214\n",
      "Epoch[7] Loss: 0.296103835105896\n",
      "Epoch[7] Loss: 0.2787315249443054\n",
      "Epoch[7] Loss: 0.3218272924423218\n",
      "Epoch[7] Loss: 0.5137939453125\n",
      "Epoch[7] Loss: 0.25362497568130493\n",
      "Epoch[7] Loss: 0.39011427760124207\n",
      "Epoch[7] Loss: 0.4037380814552307\n",
      "Epoch[7] Loss: 0.3684905767440796\n",
      "Epoch[7] Loss: 0.4983353018760681\n",
      "Training Results - Epoch: 7      Avg accuracy: 0.9233333333333333 Avg loss: 0.26287360950311023\n",
      "Validation Results - Epoch: 7      Avg accuracy: 0.9057239057239057 Avg loss: 0.26033142419776534\n",
      "Epoch[8] Loss: 0.22576400637626648\n",
      "Epoch[8] Loss: 0.2852214276790619\n",
      "Epoch[8] Loss: 0.271638959646225\n",
      "Epoch[8] Loss: 0.20046557486057281\n",
      "Epoch[8] Loss: 0.22373557090759277\n",
      "Epoch[8] Loss: 0.2926943898200989\n",
      "Epoch[8] Loss: 0.4598541557788849\n",
      "Epoch[8] Loss: 0.24679487943649292\n",
      "Epoch[8] Loss: 0.17877967655658722\n",
      "Epoch[8] Loss: 0.22196677327156067\n",
      "Epoch[8] Loss: 0.25298410654067993\n",
      "Epoch[8] Loss: 0.3328647315502167\n",
      "Epoch[8] Loss: 0.4347453713417053\n",
      "Epoch[8] Loss: 0.5219559669494629\n",
      "Epoch[8] Loss: 0.18857774138450623\n",
      "Epoch[8] Loss: 0.4142857789993286\n",
      "Epoch[8] Loss: 0.19131582975387573\n",
      "Epoch[8] Loss: 0.34875917434692383\n",
      "Epoch[8] Loss: 0.33890509605407715\n",
      "Epoch[8] Loss: 0.23185290396213531\n",
      "Epoch[8] Loss: 0.05365777015686035\n",
      "Epoch[8] Loss: 0.3417244255542755\n",
      "Epoch[8] Loss: 0.4862232804298401\n",
      "Epoch[8] Loss: 0.35304778814315796\n",
      "Epoch[8] Loss: 0.42917755246162415\n",
      "Epoch[8] Loss: 0.2773280143737793\n",
      "Epoch[8] Loss: 0.17753027379512787\n",
      "Epoch[8] Loss: 0.25694727897644043\n",
      "Epoch[8] Loss: 0.20533867180347443\n",
      "Epoch[8] Loss: 0.2870583236217499\n",
      "Epoch[8] Loss: 0.5702593922615051\n",
      "Epoch[8] Loss: 0.5536766648292542\n",
      "Epoch[8] Loss: 0.2113063931465149\n",
      "Epoch[8] Loss: 0.274740606546402\n",
      "Epoch[8] Loss: 0.2663422226905823\n",
      "Epoch[8] Loss: 0.19545434415340424\n",
      "Epoch[8] Loss: 0.24151602387428284\n",
      "Epoch[8] Loss: 0.6284502744674683\n",
      "Epoch[8] Loss: 0.206089586019516\n",
      "Epoch[8] Loss: 0.24832293391227722\n",
      "Epoch[8] Loss: 0.37234440445899963\n",
      "Epoch[8] Loss: 0.4636867642402649\n",
      "Epoch[8] Loss: 0.3020520508289337\n",
      "Epoch[8] Loss: 0.19168707728385925\n",
      "Epoch[8] Loss: 0.19473567605018616\n",
      "Epoch[8] Loss: 0.3529191315174103\n",
      "Epoch[8] Loss: 0.1521303802728653\n",
      "Training Results - Epoch: 8      Avg accuracy: 0.9306666666666666 Avg loss: 0.24522720913092294\n",
      "Validation Results - Epoch: 8      Avg accuracy: 0.9057239057239057 Avg loss: 0.2699018592384929\n",
      "Epoch[9] Loss: 0.34520772099494934\n",
      "Epoch[9] Loss: 0.1633562296628952\n",
      "Epoch[9] Loss: 0.38032087683677673\n",
      "Epoch[9] Loss: 0.501937985420227\n",
      "Epoch[9] Loss: 0.20841659605503082\n",
      "Epoch[9] Loss: 0.06609567999839783\n",
      "Epoch[9] Loss: 0.20132212340831757\n",
      "Epoch[9] Loss: 0.3205946087837219\n",
      "Epoch[9] Loss: 0.20854313671588898\n",
      "Epoch[9] Loss: 0.3052838146686554\n",
      "Epoch[9] Loss: 0.5102592706680298\n",
      "Epoch[9] Loss: 0.36096686124801636\n",
      "Epoch[9] Loss: 0.2401311695575714\n",
      "Epoch[9] Loss: 0.355776846408844\n",
      "Epoch[9] Loss: 0.26418888568878174\n",
      "Epoch[9] Loss: 0.5389515161514282\n",
      "Epoch[9] Loss: 0.543918788433075\n",
      "Epoch[9] Loss: 0.4457763731479645\n",
      "Epoch[9] Loss: 0.17318840324878693\n",
      "Epoch[9] Loss: 0.2617482542991638\n",
      "Epoch[9] Loss: 0.35158708691596985\n",
      "Epoch[9] Loss: 0.21014337241649628\n",
      "Epoch[9] Loss: 0.4015059769153595\n",
      "Epoch[9] Loss: 0.11401280760765076\n",
      "Epoch[9] Loss: 0.3501293659210205\n",
      "Epoch[9] Loss: 0.26576170325279236\n",
      "Epoch[9] Loss: 0.28227993845939636\n",
      "Epoch[9] Loss: 0.1014610230922699\n",
      "Epoch[9] Loss: 0.08155575394630432\n",
      "Epoch[9] Loss: 0.2469550520181656\n",
      "Epoch[9] Loss: 0.16110537946224213\n",
      "Epoch[9] Loss: 0.27368107438087463\n",
      "Epoch[9] Loss: 0.28237608075141907\n",
      "Epoch[9] Loss: 0.39554256200790405\n",
      "Epoch[9] Loss: 0.28286507725715637\n",
      "Epoch[9] Loss: 0.07921577990055084\n",
      "Epoch[9] Loss: 0.24582503736019135\n",
      "Epoch[9] Loss: 0.20585517585277557\n",
      "Epoch[9] Loss: 0.19835221767425537\n",
      "Epoch[9] Loss: 0.213674858212471\n",
      "Epoch[9] Loss: 0.23784296214580536\n",
      "Epoch[9] Loss: 0.3969908058643341\n",
      "Epoch[9] Loss: 0.5322927236557007\n",
      "Epoch[9] Loss: 0.15500842034816742\n",
      "Epoch[9] Loss: 0.5218249559402466\n",
      "Epoch[9] Loss: 0.2531362771987915\n",
      "Epoch[9] Loss: 0.1735192984342575\n",
      "Training Results - Epoch: 9      Avg accuracy: 0.9253333333333333 Avg loss: 0.24891017218430836\n",
      "Validation Results - Epoch: 9      Avg accuracy: 0.9158249158249159 Avg loss: 0.27075367082249036\n",
      "Epoch[10] Loss: 0.16462457180023193\n",
      "Epoch[10] Loss: 0.24333548545837402\n",
      "Epoch[10] Loss: 0.30145561695098877\n",
      "Epoch[10] Loss: 0.13307902216911316\n",
      "Epoch[10] Loss: 0.31554120779037476\n",
      "Epoch[10] Loss: 0.21787336468696594\n",
      "Epoch[10] Loss: 0.2622334957122803\n",
      "Epoch[10] Loss: 0.22173191606998444\n",
      "Epoch[10] Loss: 0.09756981581449509\n",
      "Epoch[10] Loss: 0.21461740136146545\n",
      "Epoch[10] Loss: 0.16340985894203186\n",
      "Epoch[10] Loss: 0.2691372334957123\n",
      "Epoch[10] Loss: 0.46836185455322266\n",
      "Epoch[10] Loss: 0.16982583701610565\n",
      "Epoch[10] Loss: 0.2030835896730423\n",
      "Epoch[10] Loss: 0.5581767559051514\n",
      "Epoch[10] Loss: 0.5389420986175537\n",
      "Epoch[10] Loss: 0.522379994392395\n",
      "Epoch[10] Loss: 0.29762041568756104\n",
      "Epoch[10] Loss: 0.36128175258636475\n",
      "Epoch[10] Loss: 0.23143360018730164\n",
      "Epoch[10] Loss: 0.2910005450248718\n",
      "Epoch[10] Loss: 0.2278890758752823\n",
      "Epoch[10] Loss: 0.5553462505340576\n",
      "Epoch[10] Loss: 0.08200664818286896\n",
      "Epoch[10] Loss: 0.40338432788848877\n",
      "Epoch[10] Loss: 0.12655124068260193\n",
      "Epoch[10] Loss: 0.22564423084259033\n",
      "Epoch[10] Loss: 0.4617517292499542\n",
      "Epoch[10] Loss: 0.37348923087120056\n",
      "Epoch[10] Loss: 0.3532741963863373\n",
      "Epoch[10] Loss: 0.28924766182899475\n",
      "Epoch[10] Loss: 0.2919684052467346\n",
      "Epoch[10] Loss: 0.16225022077560425\n",
      "Epoch[10] Loss: 0.06455133110284805\n",
      "Epoch[10] Loss: 0.27068620920181274\n",
      "Epoch[10] Loss: 0.24855810403823853\n",
      "Epoch[10] Loss: 0.18718473613262177\n",
      "Epoch[10] Loss: 0.22140303254127502\n",
      "Epoch[10] Loss: 0.31333011388778687\n",
      "Epoch[10] Loss: 0.44904839992523193\n",
      "Epoch[10] Loss: 0.30845907330513\n",
      "Epoch[10] Loss: 0.20168478786945343\n",
      "Epoch[10] Loss: 0.1314302235841751\n",
      "Epoch[10] Loss: 0.29937148094177246\n",
      "Epoch[10] Loss: 0.3204944133758545\n",
      "Epoch[10] Loss: 0.26261308789253235\n",
      "Training Results - Epoch: 10      Avg accuracy: 0.926 Avg loss: 0.24045582795143128\n",
      "Validation Results - Epoch: 10      Avg accuracy: 0.9090909090909091 Avg loss: 0.27016462019396953\n",
      "Epoch[11] Loss: 0.2824808955192566\n",
      "Epoch[11] Loss: 0.22325336933135986\n",
      "Epoch[11] Loss: 0.17450939118862152\n",
      "Epoch[11] Loss: 0.18076734244823456\n",
      "Epoch[11] Loss: 0.2291855365037918\n",
      "Epoch[11] Loss: 0.7185573577880859\n",
      "Epoch[11] Loss: 0.30437520146369934\n",
      "Epoch[11] Loss: 0.3093872666358948\n",
      "Epoch[11] Loss: 0.3066446781158447\n",
      "Epoch[11] Loss: 0.07657723873853683\n",
      "Epoch[11] Loss: 0.3726309835910797\n",
      "Epoch[11] Loss: 0.47090595960617065\n",
      "Epoch[11] Loss: 0.25836512446403503\n",
      "Epoch[11] Loss: 0.560667872428894\n",
      "Epoch[11] Loss: 0.49162930250167847\n",
      "Epoch[11] Loss: 0.14855286478996277\n",
      "Epoch[11] Loss: 0.07325172424316406\n",
      "Epoch[11] Loss: 0.25459015369415283\n",
      "Epoch[11] Loss: 0.45532214641571045\n",
      "Epoch[11] Loss: 0.3246173560619354\n",
      "Epoch[11] Loss: 0.31962230801582336\n",
      "Epoch[11] Loss: 0.39712369441986084\n",
      "Epoch[11] Loss: 0.12613174319267273\n",
      "Epoch[11] Loss: 0.07556335628032684\n",
      "Epoch[11] Loss: 0.352877676486969\n",
      "Epoch[11] Loss: 0.4580295979976654\n",
      "Epoch[11] Loss: 0.49554020166397095\n",
      "Epoch[11] Loss: 0.3182035982608795\n",
      "Epoch[11] Loss: 0.3976573646068573\n",
      "Epoch[11] Loss: 0.5299094319343567\n",
      "Epoch[11] Loss: 0.15566422045230865\n",
      "Epoch[11] Loss: 0.2703516483306885\n",
      "Epoch[11] Loss: 0.16493871808052063\n",
      "Epoch[11] Loss: 0.4289640784263611\n",
      "Epoch[11] Loss: 0.13926763832569122\n",
      "Epoch[11] Loss: 0.2753773331642151\n",
      "Epoch[11] Loss: 0.2802865207195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11] Loss: 0.43239232897758484\n",
      "Epoch[11] Loss: 0.183802992105484\n",
      "Epoch[11] Loss: 0.11893582344055176\n",
      "Epoch[11] Loss: 0.6563249826431274\n",
      "Epoch[11] Loss: 0.2344914972782135\n",
      "Epoch[11] Loss: 0.13182197511196136\n",
      "Epoch[11] Loss: 0.3911898732185364\n",
      "Epoch[11] Loss: 0.3828166425228119\n",
      "Epoch[11] Loss: 0.5033184885978699\n",
      "Epoch[11] Loss: 0.44583016633987427\n",
      "Training Results - Epoch: 11      Avg accuracy: 0.9273333333333333 Avg loss: 0.24989605192343395\n",
      "Validation Results - Epoch: 11      Avg accuracy: 0.9225589225589226 Avg loss: 0.26974760091264643\n",
      "Epoch[12] Loss: 0.3714364767074585\n",
      "Epoch[12] Loss: 0.18285608291625977\n",
      "Epoch[12] Loss: 0.14521992206573486\n",
      "Epoch[12] Loss: 0.5590334534645081\n",
      "Epoch[12] Loss: 0.43653902411460876\n",
      "Epoch[12] Loss: 0.4826205372810364\n",
      "Epoch[12] Loss: 0.22396765649318695\n",
      "Epoch[12] Loss: 0.22709128260612488\n",
      "Epoch[12] Loss: 0.19054800271987915\n",
      "Epoch[12] Loss: 0.402994841337204\n",
      "Epoch[12] Loss: 0.40153416991233826\n",
      "Epoch[12] Loss: 0.3418926000595093\n",
      "Epoch[12] Loss: 0.464826762676239\n",
      "Epoch[12] Loss: 0.21874411404132843\n",
      "Epoch[12] Loss: 0.3861333429813385\n",
      "Epoch[12] Loss: 0.2273033708333969\n",
      "Epoch[12] Loss: 0.20492905378341675\n",
      "Epoch[12] Loss: 0.22716185450553894\n",
      "Epoch[12] Loss: 0.31802427768707275\n",
      "Epoch[12] Loss: 0.3511336147785187\n",
      "Epoch[12] Loss: 0.2726810574531555\n",
      "Epoch[12] Loss: 0.17323344945907593\n",
      "Epoch[12] Loss: 0.27808013558387756\n",
      "Epoch[12] Loss: 0.17548403143882751\n",
      "Epoch[12] Loss: 0.09703145921230316\n",
      "Epoch[12] Loss: 0.2225959599018097\n",
      "Epoch[12] Loss: 0.1064547449350357\n",
      "Epoch[12] Loss: 0.15963459014892578\n",
      "Epoch[12] Loss: 0.3366645872592926\n",
      "Epoch[12] Loss: 0.39940863847732544\n",
      "Epoch[12] Loss: 0.2116524577140808\n",
      "Epoch[12] Loss: 0.21282602846622467\n",
      "Epoch[12] Loss: 0.624549388885498\n",
      "Epoch[12] Loss: 0.13020597398281097\n",
      "Epoch[12] Loss: 0.4337806701660156\n",
      "Epoch[12] Loss: 0.2511822581291199\n",
      "Epoch[12] Loss: 0.7429437041282654\n",
      "Epoch[12] Loss: 0.3916381299495697\n",
      "Epoch[12] Loss: 0.2920081615447998\n",
      "Epoch[12] Loss: 0.2743030786514282\n",
      "Epoch[12] Loss: 0.28507450222969055\n",
      "Epoch[12] Loss: 0.34605342149734497\n",
      "Epoch[12] Loss: 0.16221056878566742\n",
      "Epoch[12] Loss: 0.39256107807159424\n",
      "Epoch[12] Loss: 0.18477961421012878\n",
      "Epoch[12] Loss: 0.21720491349697113\n",
      "Epoch[12] Loss: 0.35150888562202454\n",
      "Training Results - Epoch: 12      Avg accuracy: 0.9233333333333333 Avg loss: 0.24845951346556347\n",
      "Validation Results - Epoch: 12      Avg accuracy: 0.9090909090909091 Avg loss: 0.2834412854929966\n",
      "Epoch[13] Loss: 0.3124627470970154\n",
      "Epoch[13] Loss: 0.24983181059360504\n",
      "Epoch[13] Loss: 0.1976616531610489\n",
      "Epoch[13] Loss: 0.4538196921348572\n",
      "Epoch[13] Loss: 0.3116965889930725\n",
      "Epoch[13] Loss: 0.48630839586257935\n",
      "Epoch[13] Loss: 0.19824659824371338\n",
      "Epoch[13] Loss: 0.10579242557287216\n",
      "Epoch[13] Loss: 0.29793432354927063\n",
      "Epoch[13] Loss: 0.351431280374527\n",
      "Epoch[13] Loss: 0.25341174006462097\n",
      "Epoch[13] Loss: 0.4475246071815491\n",
      "Epoch[13] Loss: 0.6804518699645996\n",
      "Epoch[13] Loss: 0.3278006613254547\n",
      "Epoch[13] Loss: 0.4194231927394867\n",
      "Epoch[13] Loss: 0.18724171817302704\n",
      "Epoch[13] Loss: 0.13610491156578064\n",
      "Epoch[13] Loss: 0.28491586446762085\n",
      "Epoch[13] Loss: 0.3620477020740509\n",
      "Epoch[13] Loss: 0.14116249978542328\n",
      "Epoch[13] Loss: 0.7420907020568848\n",
      "Epoch[13] Loss: 0.25493231415748596\n",
      "Epoch[13] Loss: 0.1715036779642105\n",
      "Epoch[13] Loss: 0.1415971964597702\n",
      "Epoch[13] Loss: 0.14401192963123322\n",
      "Epoch[13] Loss: 0.29728561639785767\n",
      "Epoch[13] Loss: 0.3409082293510437\n",
      "Epoch[13] Loss: 0.33564817905426025\n",
      "Epoch[13] Loss: 0.14798958599567413\n",
      "Epoch[13] Loss: 0.3563273549079895\n",
      "Epoch[13] Loss: 0.4125971794128418\n",
      "Epoch[13] Loss: 0.34884020686149597\n",
      "Epoch[13] Loss: 0.2788676619529724\n",
      "Epoch[13] Loss: 0.13342690467834473\n",
      "Epoch[13] Loss: 0.314190536737442\n",
      "Epoch[13] Loss: 0.3394884169101715\n",
      "Epoch[13] Loss: 0.2465437948703766\n",
      "Epoch[13] Loss: 0.32889145612716675\n",
      "Epoch[13] Loss: 0.2769579589366913\n",
      "Epoch[13] Loss: 0.1104481965303421\n",
      "Epoch[13] Loss: 0.27439436316490173\n",
      "Epoch[13] Loss: 0.26716113090515137\n",
      "Epoch[13] Loss: 0.557151734828949\n",
      "Epoch[13] Loss: 0.444313108921051\n",
      "Epoch[13] Loss: 0.28542473912239075\n",
      "Epoch[13] Loss: 0.4568862020969391\n",
      "Epoch[13] Loss: 0.16715934872627258\n",
      "Training Results - Epoch: 13      Avg accuracy: 0.926 Avg loss: 0.2536819663842519\n",
      "Validation Results - Epoch: 13      Avg accuracy: 0.9057239057239057 Avg loss: 0.2738124799447429\n",
      "Epoch[14] Loss: 0.14629265666007996\n",
      "Epoch[14] Loss: 0.3123301565647125\n",
      "Epoch[14] Loss: 0.26371896266937256\n",
      "Epoch[14] Loss: 0.24529266357421875\n",
      "Epoch[14] Loss: 0.18403688073158264\n",
      "Epoch[14] Loss: 0.13268254697322845\n",
      "Epoch[14] Loss: 0.37283942103385925\n",
      "Epoch[14] Loss: 0.3417474329471588\n",
      "Epoch[14] Loss: 0.30691537261009216\n",
      "Epoch[14] Loss: 0.32712703943252563\n",
      "Epoch[14] Loss: 0.201222226023674\n",
      "Epoch[14] Loss: 0.16617833077907562\n",
      "Epoch[14] Loss: 0.1607845425605774\n",
      "Epoch[14] Loss: 0.42420321702957153\n",
      "Epoch[14] Loss: 0.4446718990802765\n",
      "Epoch[14] Loss: 0.22961919009685516\n",
      "Epoch[14] Loss: 0.22505950927734375\n",
      "Epoch[14] Loss: 0.16704139113426208\n",
      "Epoch[14] Loss: 0.23476234078407288\n",
      "Epoch[14] Loss: 0.18298602104187012\n",
      "Epoch[14] Loss: 0.2316235899925232\n",
      "Epoch[14] Loss: 0.13900189101696014\n",
      "Epoch[14] Loss: 0.32167530059814453\n",
      "Epoch[14] Loss: 0.24680422246456146\n",
      "Epoch[14] Loss: 0.20462366938591003\n",
      "Epoch[14] Loss: 0.21780365705490112\n",
      "Epoch[14] Loss: 0.3675665855407715\n",
      "Epoch[14] Loss: 0.27883750200271606\n",
      "Epoch[14] Loss: 0.6853906512260437\n",
      "Epoch[14] Loss: 0.38137030601501465\n",
      "Epoch[14] Loss: 0.1925322711467743\n",
      "Epoch[14] Loss: 0.2489328682422638\n",
      "Epoch[14] Loss: 0.1624954789876938\n",
      "Epoch[14] Loss: 0.45635727047920227\n",
      "Epoch[14] Loss: 0.5849798917770386\n",
      "Epoch[14] Loss: 0.21207207441329956\n",
      "Epoch[14] Loss: 0.2989791929721832\n",
      "Epoch[14] Loss: 0.18363375961780548\n",
      "Epoch[14] Loss: 0.3670881390571594\n",
      "Epoch[14] Loss: 0.39009833335876465\n",
      "Epoch[14] Loss: 0.3894385099411011\n",
      "Epoch[14] Loss: 0.5134450197219849\n",
      "Epoch[14] Loss: 0.2857598066329956\n",
      "Epoch[14] Loss: 0.2727821469306946\n",
      "Epoch[14] Loss: 0.33453431725502014\n",
      "Epoch[14] Loss: 0.2104131430387497\n",
      "Epoch[14] Loss: 0.09604305773973465\n",
      "Training Results - Epoch: 14      Avg accuracy: 0.9293333333333333 Avg loss: 0.24484864588578542\n",
      "Validation Results - Epoch: 14      Avg accuracy: 0.898989898989899 Avg loss: 0.2852573579409307\n",
      "Epoch[15] Loss: 0.09947693347930908\n",
      "Epoch[15] Loss: 0.2560342848300934\n",
      "Epoch[15] Loss: 0.24112890660762787\n",
      "Epoch[15] Loss: 0.11444541811943054\n",
      "Epoch[15] Loss: 0.12760379910469055\n",
      "Epoch[15] Loss: 0.3498579263687134\n",
      "Epoch[15] Loss: 0.22583092749118805\n",
      "Epoch[15] Loss: 0.3906501233577728\n",
      "Epoch[15] Loss: 0.6195579767227173\n",
      "Epoch[15] Loss: 0.21001219749450684\n",
      "Epoch[15] Loss: 0.19839002192020416\n",
      "Epoch[15] Loss: 0.3993318974971771\n",
      "Epoch[15] Loss: 0.23279473185539246\n",
      "Epoch[15] Loss: 0.33645448088645935\n",
      "Epoch[15] Loss: 0.285660982131958\n",
      "Epoch[15] Loss: 0.061451300978660583\n",
      "Epoch[15] Loss: 0.3360655903816223\n",
      "Epoch[15] Loss: 0.3873172998428345\n",
      "Epoch[15] Loss: 0.35052430629730225\n",
      "Epoch[15] Loss: 0.20007973909378052\n",
      "Epoch[15] Loss: 0.10128527879714966\n",
      "Epoch[15] Loss: 0.20378924906253815\n",
      "Epoch[15] Loss: 0.2739655375480652\n",
      "Epoch[15] Loss: 0.1502646505832672\n",
      "Epoch[15] Loss: 0.09228723496198654\n",
      "Epoch[15] Loss: 0.0705721527338028\n",
      "Epoch[15] Loss: 0.17437827587127686\n",
      "Epoch[15] Loss: 0.08040061593055725\n",
      "Epoch[15] Loss: 0.21568633615970612\n",
      "Epoch[15] Loss: 0.42462432384490967\n",
      "Epoch[15] Loss: 0.29410621523857117\n",
      "Epoch[15] Loss: 0.19419166445732117\n",
      "Epoch[15] Loss: 0.152621790766716\n",
      "Epoch[15] Loss: 0.09102961421012878\n",
      "Epoch[15] Loss: 0.619277834892273\n",
      "Epoch[15] Loss: 0.38925719261169434\n",
      "Epoch[15] Loss: 0.2040114849805832\n",
      "Epoch[15] Loss: 0.3824408948421478\n",
      "Epoch[15] Loss: 0.21293722093105316\n",
      "Epoch[15] Loss: 0.21546044945716858\n",
      "Epoch[15] Loss: 0.2529004216194153\n",
      "Epoch[15] Loss: 0.23032601177692413\n",
      "Epoch[15] Loss: 0.1738012582063675\n",
      "Epoch[15] Loss: 0.25788745284080505\n",
      "Epoch[15] Loss: 0.23266613483428955\n",
      "Epoch[15] Loss: 0.13386748731136322\n",
      "Epoch[15] Loss: 0.3947473168373108\n",
      "Training Results - Epoch: 15      Avg accuracy: 0.9273333333333333 Avg loss: 0.24699009267489117\n",
      "Validation Results - Epoch: 15      Avg accuracy: 0.8956228956228957 Avg loss: 0.2860369172561851\n",
      "Epoch[16] Loss: 0.4074398875236511\n",
      "Epoch[16] Loss: 0.35364872217178345\n",
      "Epoch[16] Loss: 0.26789340376853943\n",
      "Epoch[16] Loss: 0.17416812479496002\n",
      "Epoch[16] Loss: 0.4227936267852783\n",
      "Epoch[16] Loss: 0.1741715520620346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16] Loss: 0.2136499583721161\n",
      "Epoch[16] Loss: 0.1787758469581604\n",
      "Epoch[16] Loss: 0.31964826583862305\n",
      "Epoch[16] Loss: 0.24549232423305511\n",
      "Epoch[16] Loss: 0.3464367091655731\n",
      "Epoch[16] Loss: 0.22912099957466125\n",
      "Epoch[16] Loss: 0.2970203161239624\n",
      "Epoch[16] Loss: 0.24447645246982574\n",
      "Epoch[16] Loss: 0.3667951226234436\n",
      "Epoch[16] Loss: 0.27551960945129395\n",
      "Epoch[16] Loss: 0.15468323230743408\n",
      "Epoch[16] Loss: 0.33388715982437134\n",
      "Epoch[16] Loss: 0.6220146417617798\n",
      "Epoch[16] Loss: 0.32127177715301514\n",
      "Epoch[16] Loss: 0.19033078849315643\n",
      "Epoch[16] Loss: 0.33982333540916443\n",
      "Epoch[16] Loss: 0.28538623452186584\n",
      "Epoch[16] Loss: 0.5133522152900696\n",
      "Epoch[16] Loss: 0.25155532360076904\n",
      "Epoch[16] Loss: 0.1638946235179901\n",
      "Epoch[16] Loss: 0.4722900986671448\n",
      "Epoch[16] Loss: 0.26871857047080994\n",
      "Epoch[16] Loss: 0.31385737657546997\n",
      "Epoch[16] Loss: 0.16310858726501465\n",
      "Epoch[16] Loss: 0.5319536328315735\n",
      "Epoch[16] Loss: 0.35008156299591064\n",
      "Epoch[16] Loss: 0.5218817591667175\n",
      "Epoch[16] Loss: 0.2923532724380493\n",
      "Epoch[16] Loss: 0.1842976063489914\n",
      "Epoch[16] Loss: 0.14255550503730774\n",
      "Epoch[16] Loss: 0.25051072239875793\n",
      "Epoch[16] Loss: 0.42938631772994995\n",
      "Epoch[16] Loss: 0.4555049538612366\n",
      "Epoch[16] Loss: 0.3477373719215393\n",
      "Epoch[16] Loss: 0.36786824464797974\n",
      "Epoch[16] Loss: 0.3353290259838104\n",
      "Epoch[16] Loss: 0.2978258430957794\n",
      "Epoch[16] Loss: 0.2623758316040039\n",
      "Epoch[16] Loss: 0.24039487540721893\n",
      "Epoch[16] Loss: 0.2347201257944107\n",
      "Epoch[16] Loss: 0.2096114158630371\n",
      "Training Results - Epoch: 16      Avg accuracy: 0.9313333333333333 Avg loss: 0.2412762235403061\n",
      "Validation Results - Epoch: 16      Avg accuracy: 0.898989898989899 Avg loss: 0.274897443927097\n",
      "Epoch[17] Loss: 0.48104989528656006\n",
      "Epoch[17] Loss: 0.33480575680732727\n",
      "Epoch[17] Loss: 0.1985584795475006\n",
      "Epoch[17] Loss: 0.23540399968624115\n",
      "Epoch[17] Loss: 0.30213382840156555\n",
      "Epoch[17] Loss: 0.2139797806739807\n",
      "Epoch[17] Loss: 0.36181557178497314\n",
      "Epoch[17] Loss: 0.17708241939544678\n",
      "Epoch[17] Loss: 0.16044427454471588\n",
      "Epoch[17] Loss: 0.21097129583358765\n",
      "Epoch[17] Loss: 0.18973734974861145\n",
      "Epoch[17] Loss: 0.26319271326065063\n",
      "Epoch[17] Loss: 0.37148940563201904\n",
      "Epoch[17] Loss: 0.15125654637813568\n",
      "Epoch[17] Loss: 0.3666592538356781\n",
      "Epoch[17] Loss: 0.10503038763999939\n",
      "Epoch[17] Loss: 0.10075487196445465\n",
      "Epoch[17] Loss: 0.31034883856773376\n",
      "Epoch[17] Loss: 0.16269147396087646\n",
      "Epoch[17] Loss: 0.20255683362483978\n",
      "Epoch[17] Loss: 0.35701483488082886\n",
      "Epoch[17] Loss: 0.1956714391708374\n",
      "Epoch[17] Loss: 0.306790828704834\n",
      "Epoch[17] Loss: 0.5832050442695618\n",
      "Epoch[17] Loss: 0.29725757241249084\n",
      "Epoch[17] Loss: 0.24300123751163483\n",
      "Epoch[17] Loss: 0.24681049585342407\n",
      "Epoch[17] Loss: 0.5285458564758301\n",
      "Epoch[17] Loss: 0.266422837972641\n",
      "Epoch[17] Loss: 0.3699115216732025\n",
      "Epoch[17] Loss: 0.1378914713859558\n",
      "Epoch[17] Loss: 0.18659967184066772\n",
      "Epoch[17] Loss: 0.23974408209323883\n",
      "Epoch[17] Loss: 0.5058584213256836\n",
      "Epoch[17] Loss: 0.26504433155059814\n",
      "Epoch[17] Loss: 0.28317689895629883\n",
      "Epoch[17] Loss: 0.16279982030391693\n",
      "Epoch[17] Loss: 0.23918429017066956\n",
      "Epoch[17] Loss: 0.1655912697315216\n",
      "Epoch[17] Loss: 0.21993882954120636\n",
      "Epoch[17] Loss: 0.24518170952796936\n",
      "Epoch[17] Loss: 0.27957573533058167\n",
      "Epoch[17] Loss: 0.1424373984336853\n",
      "Epoch[17] Loss: 0.36380836367607117\n",
      "Epoch[17] Loss: 0.4291096031665802\n",
      "Epoch[17] Loss: 0.2175270915031433\n",
      "Epoch[17] Loss: 0.43250057101249695\n",
      "Training Results - Epoch: 17      Avg accuracy: 0.926 Avg loss: 0.23845763941605885\n",
      "Validation Results - Epoch: 17      Avg accuracy: 0.9057239057239057 Avg loss: 0.2915589151157675\n",
      "Epoch[18] Loss: 0.41430026292800903\n",
      "Epoch[18] Loss: 0.28438639640808105\n",
      "Epoch[18] Loss: 0.34220096468925476\n",
      "Epoch[18] Loss: 0.3698546290397644\n",
      "Epoch[18] Loss: 0.3183528184890747\n",
      "Epoch[18] Loss: 0.2647981345653534\n",
      "Epoch[18] Loss: 0.16951119899749756\n",
      "Epoch[18] Loss: 0.26678287982940674\n",
      "Epoch[18] Loss: 0.27265143394470215\n",
      "Epoch[18] Loss: 0.26671892404556274\n",
      "Epoch[18] Loss: 0.3071668744087219\n",
      "Epoch[18] Loss: 0.635883092880249\n",
      "Epoch[18] Loss: 0.20765253901481628\n",
      "Epoch[18] Loss: 0.41840803623199463\n",
      "Epoch[18] Loss: 0.30662450194358826\n",
      "Epoch[18] Loss: 0.5571805238723755\n",
      "Epoch[18] Loss: 0.2050153911113739\n",
      "Epoch[18] Loss: 0.17718344926834106\n",
      "Epoch[18] Loss: 0.23612074553966522\n",
      "Epoch[18] Loss: 0.2975185811519623\n",
      "Epoch[18] Loss: 0.35288336873054504\n",
      "Epoch[18] Loss: 0.13113491237163544\n",
      "Epoch[18] Loss: 0.347906231880188\n",
      "Epoch[18] Loss: 0.07427142560482025\n",
      "Epoch[18] Loss: 0.1628701239824295\n",
      "Epoch[18] Loss: 0.26700109243392944\n",
      "Epoch[18] Loss: 0.25778087973594666\n",
      "Epoch[18] Loss: 0.2021673321723938\n",
      "Epoch[18] Loss: 0.2061859667301178\n",
      "Epoch[18] Loss: 0.17739485204219818\n",
      "Epoch[18] Loss: 0.17064455151557922\n",
      "Epoch[18] Loss: 0.36880046129226685\n",
      "Epoch[18] Loss: 0.3198080062866211\n",
      "Epoch[18] Loss: 0.21071647107601166\n",
      "Epoch[18] Loss: 0.5095696449279785\n",
      "Epoch[18] Loss: 0.14252299070358276\n",
      "Epoch[18] Loss: 0.12256745994091034\n",
      "Epoch[18] Loss: 0.30645832419395447\n",
      "Epoch[18] Loss: 0.40725845098495483\n",
      "Epoch[18] Loss: 0.500106692314148\n",
      "Epoch[18] Loss: 0.06678473949432373\n",
      "Epoch[18] Loss: 0.14333686232566833\n",
      "Epoch[18] Loss: 0.4946076571941376\n",
      "Epoch[18] Loss: 0.29071831703186035\n",
      "Epoch[18] Loss: 0.19387976825237274\n",
      "Epoch[18] Loss: 0.5280697345733643\n",
      "Epoch[18] Loss: 0.17217160761356354\n",
      "Training Results - Epoch: 18      Avg accuracy: 0.9266666666666666 Avg loss: 0.24130893095334371\n",
      "Validation Results - Epoch: 18      Avg accuracy: 0.9057239057239057 Avg loss: 0.27695401349051635\n",
      "Epoch[19] Loss: 0.10428781807422638\n",
      "Epoch[19] Loss: 0.11997310817241669\n",
      "Epoch[19] Loss: 0.25299376249313354\n",
      "Epoch[19] Loss: 0.2896498739719391\n",
      "Epoch[19] Loss: 0.46871164441108704\n",
      "Epoch[19] Loss: 0.27859559655189514\n",
      "Epoch[19] Loss: 0.24039074778556824\n",
      "Epoch[19] Loss: 0.3700770139694214\n",
      "Epoch[19] Loss: 0.6391977071762085\n",
      "Epoch[19] Loss: 0.3241327404975891\n",
      "Epoch[19] Loss: 0.3198567032814026\n",
      "Epoch[19] Loss: 0.4016581177711487\n",
      "Epoch[19] Loss: 0.26904305815696716\n",
      "Epoch[19] Loss: 0.4037392735481262\n",
      "Epoch[19] Loss: 0.20030437409877777\n",
      "Epoch[19] Loss: 0.38502034544944763\n",
      "Epoch[19] Loss: 0.2954307794570923\n",
      "Epoch[19] Loss: 0.5260127782821655\n",
      "Epoch[19] Loss: 0.3417491316795349\n",
      "Epoch[19] Loss: 0.11420398950576782\n",
      "Epoch[19] Loss: 0.22480885684490204\n",
      "Epoch[19] Loss: 0.26195019483566284\n",
      "Epoch[19] Loss: 0.21452456712722778\n",
      "Epoch[19] Loss: 0.2815503776073456\n",
      "Epoch[19] Loss: 0.103658027946949\n",
      "Epoch[19] Loss: 0.7416524887084961\n",
      "Epoch[19] Loss: 0.24204008281230927\n",
      "Epoch[19] Loss: 0.20187371969223022\n",
      "Epoch[19] Loss: 0.2623182237148285\n",
      "Epoch[19] Loss: 0.4578070044517517\n",
      "Epoch[19] Loss: 0.4350045621395111\n",
      "Epoch[19] Loss: 0.44712454080581665\n",
      "Epoch[19] Loss: 0.2306334227323532\n",
      "Epoch[19] Loss: 0.19201147556304932\n",
      "Epoch[19] Loss: 0.5056037306785583\n",
      "Epoch[19] Loss: 0.5115858316421509\n",
      "Epoch[19] Loss: 0.39893409609794617\n",
      "Epoch[19] Loss: 0.12420514225959778\n",
      "Epoch[19] Loss: 0.34049949049949646\n",
      "Epoch[19] Loss: 0.22494876384735107\n",
      "Epoch[19] Loss: 0.23131287097930908\n",
      "Epoch[19] Loss: 0.38672545552253723\n",
      "Epoch[19] Loss: 0.2507675290107727\n",
      "Epoch[19] Loss: 0.1667000651359558\n",
      "Epoch[19] Loss: 0.14788386225700378\n",
      "Epoch[19] Loss: 0.15231135487556458\n",
      "Epoch[19] Loss: 0.18932697176933289\n",
      "Training Results - Epoch: 19      Avg accuracy: 0.9253333333333333 Avg loss: 0.24445115371545156\n",
      "Validation Results - Epoch: 19      Avg accuracy: 0.8956228956228957 Avg loss: 0.2849264854333216\n",
      "Epoch[20] Loss: 0.28686651587486267\n",
      "Epoch[20] Loss: 0.33789658546447754\n",
      "Epoch[20] Loss: 0.385437935590744\n",
      "Epoch[20] Loss: 0.14306820929050446\n",
      "Epoch[20] Loss: 0.29455381631851196\n",
      "Epoch[20] Loss: 0.43029940128326416\n",
      "Epoch[20] Loss: 0.31955212354660034\n",
      "Epoch[20] Loss: 0.3062521815299988\n",
      "Epoch[20] Loss: 0.34697479009628296\n",
      "Epoch[20] Loss: 0.21079574525356293\n",
      "Epoch[20] Loss: 0.15345719456672668\n",
      "Epoch[20] Loss: 0.1818283647298813\n",
      "Epoch[20] Loss: 0.2596762180328369\n",
      "Epoch[20] Loss: 0.11810918897390366\n",
      "Epoch[20] Loss: 0.2807157337665558\n",
      "Epoch[20] Loss: 0.22197553515434265\n",
      "Epoch[20] Loss: 0.33419471979141235\n",
      "Epoch[20] Loss: 0.2240891307592392\n",
      "Epoch[20] Loss: 0.19730302691459656\n",
      "Epoch[20] Loss: 0.36645448207855225\n",
      "Epoch[20] Loss: 0.4438839256763458\n",
      "Epoch[20] Loss: 0.25420549511909485\n",
      "Epoch[20] Loss: 0.4684009253978729\n",
      "Epoch[20] Loss: 0.3624264895915985\n",
      "Epoch[20] Loss: 0.28395044803619385\n",
      "Epoch[20] Loss: 0.3530464768409729\n",
      "Epoch[20] Loss: 0.4690624177455902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[20] Loss: 0.06656014919281006\n",
      "Epoch[20] Loss: 0.31869590282440186\n",
      "Epoch[20] Loss: 0.13513216376304626\n",
      "Epoch[20] Loss: 0.20831802487373352\n",
      "Epoch[20] Loss: 0.19994470477104187\n",
      "Epoch[20] Loss: 0.20981760323047638\n",
      "Epoch[20] Loss: 0.1479162871837616\n",
      "Epoch[20] Loss: 0.16692739725112915\n",
      "Epoch[20] Loss: 0.44087108969688416\n",
      "Epoch[20] Loss: 0.3257027566432953\n",
      "Epoch[20] Loss: 0.2049408257007599\n",
      "Epoch[20] Loss: 0.2959941625595093\n",
      "Epoch[20] Loss: 0.591494083404541\n",
      "Epoch[20] Loss: 0.2542203664779663\n",
      "Epoch[20] Loss: 0.21102206408977509\n",
      "Epoch[20] Loss: 0.3037240207195282\n",
      "Epoch[20] Loss: 0.14780288934707642\n",
      "Epoch[20] Loss: 0.19701874256134033\n",
      "Epoch[20] Loss: 0.3833559453487396\n",
      "Epoch[20] Loss: 0.22409047186374664\n",
      "Training Results - Epoch: 20      Avg accuracy: 0.9186666666666666 Avg loss: 0.24671165267626444\n",
      "Validation Results - Epoch: 20      Avg accuracy: 0.9057239057239057 Avg loss: 0.2839125065691142\n",
      "Epoch[21] Loss: 0.20659761130809784\n",
      "Epoch[21] Loss: 0.6654418110847473\n",
      "Epoch[21] Loss: 0.40469181537628174\n",
      "Epoch[21] Loss: 0.21904195845127106\n",
      "Epoch[21] Loss: 0.25575581192970276\n",
      "Epoch[21] Loss: 0.11205495893955231\n",
      "Epoch[21] Loss: 0.11427924036979675\n",
      "Epoch[21] Loss: 0.331260621547699\n",
      "Epoch[21] Loss: 0.17505665123462677\n",
      "Epoch[21] Loss: 0.17588292062282562\n",
      "Epoch[21] Loss: 0.3670145571231842\n",
      "Epoch[21] Loss: 0.3656615912914276\n",
      "Epoch[21] Loss: 0.20183727145195007\n",
      "Epoch[21] Loss: 0.18488188087940216\n",
      "Epoch[21] Loss: 0.2808114290237427\n",
      "Epoch[21] Loss: 0.5666444301605225\n",
      "Epoch[21] Loss: 0.5451846122741699\n",
      "Epoch[21] Loss: 0.24423639476299286\n",
      "Epoch[21] Loss: 0.253235399723053\n",
      "Epoch[21] Loss: 0.14684738218784332\n",
      "Epoch[21] Loss: 0.11808310449123383\n",
      "Epoch[21] Loss: 0.45189088582992554\n",
      "Epoch[21] Loss: 0.5595941543579102\n",
      "Epoch[21] Loss: 0.23583024740219116\n",
      "Epoch[21] Loss: 0.3512635827064514\n",
      "Epoch[21] Loss: 0.11524855345487595\n",
      "Epoch[21] Loss: 0.35801464319229126\n",
      "Epoch[21] Loss: 0.2620652914047241\n",
      "Epoch[21] Loss: 0.27363312244415283\n",
      "Epoch[21] Loss: 0.17829613387584686\n",
      "Epoch[21] Loss: 0.3232317268848419\n",
      "Epoch[21] Loss: 0.3560307025909424\n",
      "Epoch[21] Loss: 0.5765416622161865\n",
      "Epoch[21] Loss: 0.2149813026189804\n",
      "Epoch[21] Loss: 0.39919692277908325\n",
      "Epoch[21] Loss: 0.40779387950897217\n",
      "Epoch[21] Loss: 0.10827501118183136\n",
      "Epoch[21] Loss: 0.35186395049095154\n",
      "Epoch[21] Loss: 0.372965544462204\n",
      "Epoch[21] Loss: 0.29940691590309143\n",
      "Epoch[21] Loss: 0.468362957239151\n",
      "Epoch[21] Loss: 0.27567267417907715\n",
      "Epoch[21] Loss: 0.1710469275712967\n",
      "Epoch[21] Loss: 0.341425359249115\n",
      "Epoch[21] Loss: 0.20610411465168\n",
      "Epoch[21] Loss: 0.3443233370780945\n",
      "Epoch[21] Loss: 0.12217611074447632\n",
      "Training Results - Epoch: 21      Avg accuracy: 0.9233333333333333 Avg loss: 0.24636491119861603\n",
      "Validation Results - Epoch: 21      Avg accuracy: 0.9158249158249159 Avg loss: 0.2738009712310753\n",
      "Epoch[22] Loss: 0.3312561511993408\n",
      "Epoch[22] Loss: 0.3456348776817322\n",
      "Epoch[22] Loss: 0.3825041651725769\n",
      "Epoch[22] Loss: 0.31957438588142395\n",
      "Epoch[22] Loss: 0.23873232305049896\n",
      "Epoch[22] Loss: 0.09054972231388092\n",
      "Epoch[22] Loss: 0.22614827752113342\n",
      "Epoch[22] Loss: 0.12974849343299866\n",
      "Epoch[22] Loss: 0.052549198269844055\n",
      "Epoch[22] Loss: 0.3524589538574219\n",
      "Epoch[22] Loss: 0.3326500356197357\n",
      "Epoch[22] Loss: 0.09088748693466187\n",
      "Epoch[22] Loss: 0.30661746859550476\n",
      "Epoch[22] Loss: 0.17051535844802856\n",
      "Epoch[22] Loss: 0.16638317704200745\n",
      "Epoch[22] Loss: 0.39155471324920654\n",
      "Epoch[22] Loss: 0.17328958213329315\n",
      "Epoch[22] Loss: 0.5557097792625427\n",
      "Epoch[22] Loss: 0.23814406991004944\n",
      "Epoch[22] Loss: 0.23993605375289917\n",
      "Epoch[22] Loss: 0.3534626364707947\n",
      "Epoch[22] Loss: 0.1713760495185852\n",
      "Epoch[22] Loss: 0.35837018489837646\n",
      "Epoch[22] Loss: 0.11677522957324982\n",
      "Epoch[22] Loss: 0.20797701179981232\n",
      "Epoch[22] Loss: 0.3834153115749359\n",
      "Epoch[22] Loss: 0.245285302400589\n",
      "Epoch[22] Loss: 0.4475114345550537\n",
      "Epoch[22] Loss: 0.3435293436050415\n",
      "Epoch[22] Loss: 0.33585476875305176\n",
      "Epoch[22] Loss: 0.2528480291366577\n",
      "Epoch[22] Loss: 0.3900448977947235\n",
      "Epoch[22] Loss: 0.42330312728881836\n",
      "Epoch[22] Loss: 0.509783148765564\n",
      "Epoch[22] Loss: 0.21943031251430511\n",
      "Epoch[22] Loss: 0.298409640789032\n",
      "Epoch[22] Loss: 0.40948817133903503\n",
      "Epoch[22] Loss: 0.29588836431503296\n",
      "Epoch[22] Loss: 0.3386920094490051\n",
      "Epoch[22] Loss: 0.3040877878665924\n",
      "Epoch[22] Loss: 0.21792857348918915\n",
      "Epoch[22] Loss: 0.13084375858306885\n",
      "Epoch[22] Loss: 0.49957525730133057\n",
      "Epoch[22] Loss: 0.3120660185813904\n",
      "Epoch[22] Loss: 0.4764232039451599\n",
      "Epoch[22] Loss: 0.24949944019317627\n",
      "Epoch[22] Loss: 0.2538696229457855\n",
      "Training Results - Epoch: 22      Avg accuracy: 0.926 Avg loss: 0.23774940502643585\n",
      "Validation Results - Epoch: 22      Avg accuracy: 0.898989898989899 Avg loss: 0.2812548947053325\n",
      "Epoch[23] Loss: 0.16608662903308868\n",
      "Epoch[23] Loss: 0.17836758494377136\n",
      "Epoch[23] Loss: 0.27132272720336914\n",
      "Epoch[23] Loss: 0.32648009061813354\n",
      "Epoch[23] Loss: 0.3814716935157776\n",
      "Epoch[23] Loss: 0.11135925352573395\n",
      "Epoch[23] Loss: 0.1717885583639145\n",
      "Epoch[23] Loss: 0.3065375089645386\n",
      "Epoch[23] Loss: 0.11074546724557877\n",
      "Epoch[23] Loss: 0.1165991872549057\n",
      "Epoch[23] Loss: 0.337371826171875\n",
      "Epoch[23] Loss: 0.14049145579338074\n",
      "Epoch[23] Loss: 0.5632606744766235\n",
      "Epoch[23] Loss: 0.2509014308452606\n",
      "Epoch[23] Loss: 0.7386502027511597\n",
      "Epoch[23] Loss: 0.31654903292655945\n",
      "Epoch[23] Loss: 0.18208838999271393\n",
      "Epoch[23] Loss: 0.2349335253238678\n",
      "Epoch[23] Loss: 0.2788463234901428\n",
      "Epoch[23] Loss: 0.21499107778072357\n",
      "Epoch[23] Loss: 0.30007728934288025\n",
      "Epoch[23] Loss: 0.32435157895088196\n",
      "Epoch[23] Loss: 0.22649653255939484\n",
      "Epoch[23] Loss: 0.3836224675178528\n",
      "Epoch[23] Loss: 0.18443609774112701\n",
      "Epoch[23] Loss: 0.37629055976867676\n",
      "Epoch[23] Loss: 0.09452711045742035\n",
      "Epoch[23] Loss: 0.2439216673374176\n",
      "Epoch[23] Loss: 0.22247038781642914\n",
      "Epoch[23] Loss: 0.30411407351493835\n",
      "Epoch[23] Loss: 0.4956267178058624\n",
      "Epoch[23] Loss: 0.1797017902135849\n",
      "Epoch[23] Loss: 0.2751128375530243\n",
      "Epoch[23] Loss: 0.3941824734210968\n",
      "Epoch[23] Loss: 0.36894890666007996\n",
      "Epoch[23] Loss: 0.10470463335514069\n",
      "Epoch[23] Loss: 0.41352707147598267\n",
      "Epoch[23] Loss: 0.3708001971244812\n",
      "Epoch[23] Loss: 0.3313218653202057\n",
      "Epoch[23] Loss: 0.2577292323112488\n",
      "Epoch[23] Loss: 0.31876543164253235\n",
      "Epoch[23] Loss: 0.31641924381256104\n",
      "Epoch[23] Loss: 0.24202758073806763\n",
      "Epoch[23] Loss: 0.436778724193573\n",
      "Epoch[23] Loss: 0.18855325877666473\n",
      "Epoch[23] Loss: 0.4910355806350708\n",
      "Epoch[23] Loss: 0.35421815514564514\n",
      "Training Results - Epoch: 23      Avg accuracy: 0.9293333333333333 Avg loss: 0.23696361327171325\n",
      "Validation Results - Epoch: 23      Avg accuracy: 0.9057239057239057 Avg loss: 0.2775884348936755\n",
      "Epoch[24] Loss: 0.43744808435440063\n",
      "Epoch[24] Loss: 0.20425817370414734\n",
      "Epoch[24] Loss: 0.30665266513824463\n",
      "Epoch[24] Loss: 0.2955692708492279\n",
      "Epoch[24] Loss: 0.1740633100271225\n",
      "Epoch[24] Loss: 0.2632165551185608\n",
      "Epoch[24] Loss: 0.35061782598495483\n",
      "Epoch[24] Loss: 0.28820547461509705\n",
      "Epoch[24] Loss: 0.4668787717819214\n",
      "Epoch[24] Loss: 0.332275390625\n",
      "Epoch[24] Loss: 0.266884982585907\n",
      "Epoch[24] Loss: 0.5090969800949097\n",
      "Epoch[24] Loss: 0.10384992510080338\n",
      "Epoch[24] Loss: 0.12794673442840576\n",
      "Epoch[24] Loss: 0.21508337557315826\n",
      "Epoch[24] Loss: 0.30837807059288025\n",
      "Epoch[24] Loss: 0.16339769959449768\n",
      "Epoch[24] Loss: 0.27286040782928467\n",
      "Epoch[24] Loss: 0.1628407984972\n",
      "Epoch[24] Loss: 0.2781217396259308\n",
      "Epoch[24] Loss: 0.49757707118988037\n",
      "Epoch[24] Loss: 0.25337183475494385\n",
      "Epoch[24] Loss: 0.44147542119026184\n",
      "Epoch[24] Loss: 0.3196897506713867\n",
      "Epoch[24] Loss: 0.1425502747297287\n",
      "Epoch[24] Loss: 0.43671101331710815\n",
      "Epoch[24] Loss: 0.4642573893070221\n",
      "Epoch[24] Loss: 0.08788393437862396\n",
      "Epoch[24] Loss: 0.11901070922613144\n",
      "Epoch[24] Loss: 0.3659741282463074\n",
      "Epoch[24] Loss: 0.2861846387386322\n",
      "Epoch[24] Loss: 0.21670331060886383\n",
      "Epoch[24] Loss: 0.4276123642921448\n",
      "Epoch[24] Loss: 0.26577892899513245\n",
      "Epoch[24] Loss: 0.11820012331008911\n",
      "Epoch[24] Loss: 0.21695762872695923\n",
      "Epoch[24] Loss: 0.3192853331565857\n",
      "Epoch[24] Loss: 0.370744913816452\n",
      "Epoch[24] Loss: 0.3401535451412201\n",
      "Epoch[24] Loss: 0.2745360732078552\n",
      "Epoch[24] Loss: 0.07044269144535065\n",
      "Epoch[24] Loss: 0.21382613480091095\n",
      "Epoch[24] Loss: 0.184753879904747\n",
      "Epoch[24] Loss: 0.1575254201889038\n",
      "Epoch[24] Loss: 0.32120755314826965\n",
      "Epoch[24] Loss: 0.5574730634689331\n",
      "Epoch[24] Loss: 0.21260149776935577\n",
      "Training Results - Epoch: 24      Avg accuracy: 0.9233333333333333 Avg loss: 0.23717059568564097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch: 24      Avg accuracy: 0.9023569023569024 Avg loss: 0.2763750498945063\n",
      "Epoch[25] Loss: 0.253757119178772\n",
      "Epoch[25] Loss: 0.27472516894340515\n",
      "Epoch[25] Loss: 0.29579317569732666\n",
      "Epoch[25] Loss: 0.3687126934528351\n",
      "Epoch[25] Loss: 0.16881795227527618\n",
      "Epoch[25] Loss: 0.1958083212375641\n",
      "Epoch[25] Loss: 0.3660332262516022\n",
      "Epoch[25] Loss: 0.2362867295742035\n",
      "Epoch[25] Loss: 0.24229297041893005\n",
      "Epoch[25] Loss: 0.4453273117542267\n",
      "Epoch[25] Loss: 0.18428705632686615\n",
      "Epoch[25] Loss: 0.3888663053512573\n",
      "Epoch[25] Loss: 0.38330376148223877\n",
      "Epoch[25] Loss: 0.2134227454662323\n",
      "Epoch[25] Loss: 0.2426822930574417\n",
      "Epoch[25] Loss: 0.25909584760665894\n",
      "Epoch[25] Loss: 0.2583329677581787\n",
      "Epoch[25] Loss: 0.3897686302661896\n",
      "Epoch[25] Loss: 0.3094560205936432\n",
      "Epoch[25] Loss: 0.09580239653587341\n",
      "Epoch[25] Loss: 0.2875731587409973\n",
      "Epoch[25] Loss: 0.3141641914844513\n",
      "Epoch[25] Loss: 0.17707301676273346\n",
      "Epoch[25] Loss: 0.20074188709259033\n",
      "Epoch[25] Loss: 0.1354796439409256\n",
      "Epoch[25] Loss: 0.2745766043663025\n",
      "Epoch[25] Loss: 0.10528448224067688\n",
      "Epoch[25] Loss: 0.3961240351200104\n",
      "Epoch[25] Loss: 0.3193485140800476\n",
      "Epoch[25] Loss: 0.07368902862071991\n",
      "Epoch[25] Loss: 0.382453054189682\n",
      "Epoch[25] Loss: 0.37660080194473267\n",
      "Epoch[25] Loss: 0.3559938967227936\n",
      "Epoch[25] Loss: 0.38662201166152954\n",
      "Epoch[25] Loss: 0.18316015601158142\n",
      "Epoch[25] Loss: 0.44718071818351746\n",
      "Epoch[25] Loss: 0.12056312710046768\n",
      "Epoch[25] Loss: 0.2743985056877136\n",
      "Epoch[25] Loss: 0.37682536244392395\n",
      "Epoch[25] Loss: 0.4106866717338562\n",
      "Epoch[25] Loss: 0.4353134334087372\n",
      "Epoch[25] Loss: 0.1596257984638214\n",
      "Epoch[25] Loss: 0.1277904212474823\n",
      "Epoch[25] Loss: 0.5101931691169739\n",
      "Epoch[25] Loss: 0.3913499116897583\n",
      "Epoch[25] Loss: 0.15536697208881378\n",
      "Epoch[25] Loss: 0.22982193529605865\n",
      "Training Results - Epoch: 25      Avg accuracy: 0.93 Avg loss: 0.24700758751233418\n",
      "Validation Results - Epoch: 25      Avg accuracy: 0.898989898989899 Avg loss: 0.2870402526775193\n",
      "Epoch[26] Loss: 0.46236151456832886\n",
      "Epoch[26] Loss: 0.3821996748447418\n",
      "Epoch[26] Loss: 0.18096311390399933\n",
      "Epoch[26] Loss: 0.2921387553215027\n",
      "Epoch[26] Loss: 0.15729761123657227\n",
      "Epoch[26] Loss: 0.5572519302368164\n",
      "Epoch[26] Loss: 0.29506585001945496\n",
      "Epoch[26] Loss: 0.0485781729221344\n",
      "Epoch[26] Loss: 0.3004738390445709\n",
      "Epoch[26] Loss: 0.29790815711021423\n",
      "Epoch[26] Loss: 0.20102466642856598\n",
      "Epoch[26] Loss: 0.3171384334564209\n",
      "Epoch[26] Loss: 0.1636686772108078\n",
      "Epoch[26] Loss: 0.31266599893569946\n",
      "Epoch[26] Loss: 0.435125470161438\n",
      "Epoch[26] Loss: 0.3078311085700989\n",
      "Epoch[26] Loss: 0.08657597005367279\n",
      "Epoch[26] Loss: 0.21790552139282227\n",
      "Epoch[26] Loss: 0.13821037113666534\n",
      "Epoch[26] Loss: 0.2627532184123993\n",
      "Epoch[26] Loss: 0.13887497782707214\n",
      "Epoch[26] Loss: 0.2438739836215973\n",
      "Epoch[26] Loss: 0.3278196156024933\n",
      "Epoch[26] Loss: 0.24011117219924927\n",
      "Epoch[26] Loss: 0.27943921089172363\n",
      "Epoch[26] Loss: 0.1779002994298935\n",
      "Epoch[26] Loss: 0.15006400644779205\n",
      "Epoch[26] Loss: 0.15822729468345642\n",
      "Epoch[26] Loss: 0.07819554954767227\n",
      "Epoch[26] Loss: 0.31569138169288635\n",
      "Epoch[26] Loss: 0.1717682182788849\n",
      "Epoch[26] Loss: 0.4632241129875183\n",
      "Epoch[26] Loss: 0.38012635707855225\n",
      "Epoch[26] Loss: 0.27278557419776917\n",
      "Epoch[26] Loss: 0.280800461769104\n",
      "Epoch[26] Loss: 0.3332476019859314\n",
      "Epoch[26] Loss: 0.2635722756385803\n",
      "Epoch[26] Loss: 0.21126267313957214\n",
      "Epoch[26] Loss: 0.11022305488586426\n",
      "Epoch[26] Loss: 0.3918977975845337\n",
      "Epoch[26] Loss: 0.1549595296382904\n",
      "Epoch[26] Loss: 0.6849799752235413\n",
      "Epoch[26] Loss: 0.4976973831653595\n",
      "Epoch[26] Loss: 0.3860727548599243\n",
      "Epoch[26] Loss: 0.3350965976715088\n",
      "Epoch[26] Loss: 0.21898609399795532\n",
      "Epoch[26] Loss: 0.30131182074546814\n",
      "Training Results - Epoch: 26      Avg accuracy: 0.924 Avg loss: 0.24495901091893513\n",
      "Validation Results - Epoch: 26      Avg accuracy: 0.898989898989899 Avg loss: 0.2879916756642788\n",
      "Epoch[27] Loss: 0.34656256437301636\n",
      "Epoch[27] Loss: 0.1955689936876297\n",
      "Epoch[27] Loss: 0.2659617066383362\n",
      "Epoch[27] Loss: 0.16578815877437592\n",
      "Epoch[27] Loss: 0.31450051069259644\n",
      "Epoch[27] Loss: 0.4608005881309509\n",
      "Epoch[27] Loss: 0.32622629404067993\n",
      "Epoch[27] Loss: 0.3447682857513428\n",
      "Epoch[27] Loss: 0.42878150939941406\n",
      "Epoch[27] Loss: 0.2251213937997818\n",
      "Epoch[27] Loss: 0.31670764088630676\n",
      "Epoch[27] Loss: 0.2587627172470093\n",
      "Epoch[27] Loss: 0.24934329092502594\n",
      "Epoch[27] Loss: 0.20937217772006989\n",
      "Epoch[27] Loss: 0.22486217319965363\n",
      "Epoch[27] Loss: 0.31065285205841064\n",
      "Epoch[27] Loss: 0.21055418252944946\n",
      "Epoch[27] Loss: 0.44672971963882446\n",
      "Epoch[27] Loss: 0.36496037244796753\n",
      "Epoch[27] Loss: 0.2474958747625351\n",
      "Epoch[27] Loss: 0.1576368808746338\n",
      "Epoch[27] Loss: 0.44675371050834656\n",
      "Epoch[27] Loss: 0.45253485441207886\n",
      "Epoch[27] Loss: 0.2441219538450241\n",
      "Epoch[27] Loss: 0.3958108425140381\n",
      "Epoch[27] Loss: 0.19319142401218414\n",
      "Epoch[27] Loss: 0.6678420305252075\n",
      "Epoch[27] Loss: 0.3448743224143982\n",
      "Epoch[27] Loss: 0.12447595596313477\n",
      "Epoch[27] Loss: 0.41982004046440125\n",
      "Epoch[27] Loss: 0.10883986204862595\n",
      "Epoch[27] Loss: 0.3322248160839081\n",
      "Epoch[27] Loss: 0.37948742508888245\n",
      "Epoch[27] Loss: 0.5417630672454834\n",
      "Epoch[27] Loss: 0.13650323450565338\n",
      "Epoch[27] Loss: 0.237614706158638\n",
      "Epoch[27] Loss: 0.23631015419960022\n",
      "Epoch[27] Loss: 0.39220887422561646\n",
      "Epoch[27] Loss: 0.25418537855148315\n",
      "Epoch[27] Loss: 0.2717312276363373\n",
      "Epoch[27] Loss: 0.26221612095832825\n",
      "Epoch[27] Loss: 0.40277257561683655\n",
      "Epoch[27] Loss: 0.23319154977798462\n",
      "Epoch[27] Loss: 0.44059354066848755\n",
      "Epoch[27] Loss: 0.1597238928079605\n",
      "Epoch[27] Loss: 0.17673027515411377\n",
      "Epoch[27] Loss: 0.27310267090797424\n",
      "Training Results - Epoch: 27      Avg accuracy: 0.926 Avg loss: 0.23581759083271026\n",
      "Validation Results - Epoch: 27      Avg accuracy: 0.8956228956228957 Avg loss: 0.2962929308414459\n",
      "Epoch[28] Loss: 0.25282102823257446\n",
      "Epoch[28] Loss: 0.19992108643054962\n",
      "Epoch[28] Loss: 0.17971006035804749\n",
      "Epoch[28] Loss: 0.15679135918617249\n",
      "Epoch[28] Loss: 0.30562031269073486\n",
      "Epoch[28] Loss: 0.2113305628299713\n",
      "Epoch[28] Loss: 0.23973886668682098\n",
      "Epoch[28] Loss: 0.19933541119098663\n",
      "Epoch[28] Loss: 0.4385247826576233\n",
      "Epoch[28] Loss: 0.05969351530075073\n",
      "Epoch[28] Loss: 0.24474935233592987\n",
      "Epoch[28] Loss: 0.22849059104919434\n",
      "Epoch[28] Loss: 0.2625662684440613\n",
      "Epoch[28] Loss: 0.11513812839984894\n",
      "Epoch[28] Loss: 0.28082582354545593\n",
      "Epoch[28] Loss: 0.29706844687461853\n",
      "Epoch[28] Loss: 0.44623929262161255\n",
      "Epoch[28] Loss: 0.37614575028419495\n",
      "Epoch[28] Loss: 0.16098763048648834\n",
      "Epoch[28] Loss: 0.3391804099082947\n",
      "Epoch[28] Loss: 0.3329044580459595\n",
      "Epoch[28] Loss: 0.331238329410553\n",
      "Epoch[28] Loss: 0.43649062514305115\n",
      "Epoch[28] Loss: 0.16876636445522308\n",
      "Epoch[28] Loss: 0.2500443458557129\n",
      "Epoch[28] Loss: 0.33647042512893677\n",
      "Epoch[28] Loss: 0.34910351037979126\n",
      "Epoch[28] Loss: 0.35589227080345154\n",
      "Epoch[28] Loss: 0.2573243975639343\n",
      "Epoch[28] Loss: 0.12594953179359436\n",
      "Epoch[28] Loss: 0.28631356358528137\n",
      "Epoch[28] Loss: 0.20199483633041382\n",
      "Epoch[28] Loss: 0.25392115116119385\n",
      "Epoch[28] Loss: 0.10098912566900253\n",
      "Epoch[28] Loss: 0.21860986948013306\n",
      "Epoch[28] Loss: 0.1981142908334732\n",
      "Epoch[28] Loss: 0.19583430886268616\n",
      "Epoch[28] Loss: 0.2584800720214844\n",
      "Epoch[28] Loss: 0.23616158962249756\n",
      "Epoch[28] Loss: 0.16879655420780182\n",
      "Epoch[28] Loss: 0.22409240901470184\n",
      "Epoch[28] Loss: 0.39172908663749695\n",
      "Epoch[28] Loss: 0.4456876218318939\n",
      "Epoch[28] Loss: 0.5592026710510254\n",
      "Epoch[28] Loss: 0.2600609362125397\n",
      "Epoch[28] Loss: 0.24077551066875458\n",
      "Epoch[28] Loss: 0.37935155630111694\n",
      "Training Results - Epoch: 28      Avg accuracy: 0.9266666666666666 Avg loss: 0.23413328262170155\n",
      "Validation Results - Epoch: 28      Avg accuracy: 0.8956228956228957 Avg loss: 0.279512502529003\n",
      "Epoch[29] Loss: 0.3612668216228485\n",
      "Epoch[29] Loss: 0.19581370055675507\n",
      "Epoch[29] Loss: 0.3181346654891968\n",
      "Epoch[29] Loss: 0.24892491102218628\n",
      "Epoch[29] Loss: 0.1774243265390396\n",
      "Epoch[29] Loss: 0.2640075087547302\n",
      "Epoch[29] Loss: 0.1867293417453766\n",
      "Epoch[29] Loss: 0.32419121265411377\n",
      "Epoch[29] Loss: 0.22591568529605865\n",
      "Epoch[29] Loss: 0.19510117173194885\n",
      "Epoch[29] Loss: 0.300232470035553\n",
      "Epoch[29] Loss: 0.22354193031787872\n",
      "Epoch[29] Loss: 0.2883903980255127\n",
      "Epoch[29] Loss: 0.28646141290664673\n",
      "Epoch[29] Loss: 0.2558540999889374\n",
      "Epoch[29] Loss: 0.20167842507362366\n",
      "Epoch[29] Loss: 0.2758648991584778\n",
      "Epoch[29] Loss: 0.2692601680755615\n",
      "Epoch[29] Loss: 0.2932717800140381\n",
      "Epoch[29] Loss: 0.37761953473091125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29] Loss: 0.5061146020889282\n",
      "Epoch[29] Loss: 0.16602221131324768\n",
      "Epoch[29] Loss: 0.15510408580303192\n",
      "Epoch[29] Loss: 0.10561282932758331\n",
      "Epoch[29] Loss: 0.22776439785957336\n",
      "Epoch[29] Loss: 0.5330411791801453\n",
      "Epoch[29] Loss: 0.2330559939146042\n",
      "Epoch[29] Loss: 0.2304699420928955\n",
      "Epoch[29] Loss: 0.6882466077804565\n",
      "Epoch[29] Loss: 0.4806562662124634\n",
      "Epoch[29] Loss: 0.1546216756105423\n",
      "Epoch[29] Loss: 0.1557958424091339\n",
      "Epoch[29] Loss: 0.4370015263557434\n",
      "Epoch[29] Loss: 0.29960325360298157\n",
      "Epoch[29] Loss: 0.5832122564315796\n",
      "Epoch[29] Loss: 0.21321099996566772\n",
      "Epoch[29] Loss: 0.11215807497501373\n",
      "Epoch[29] Loss: 0.3877201974391937\n",
      "Epoch[29] Loss: 0.17748835682868958\n",
      "Epoch[29] Loss: 0.18808218836784363\n",
      "Epoch[29] Loss: 0.5275797843933105\n",
      "Epoch[29] Loss: 0.28289029002189636\n",
      "Epoch[29] Loss: 0.4505458474159241\n",
      "Epoch[29] Loss: 0.21846464276313782\n",
      "Epoch[29] Loss: 0.2818896472454071\n",
      "Epoch[29] Loss: 0.2564638555049896\n",
      "Epoch[29] Loss: 0.6347087025642395\n",
      "Training Results - Epoch: 29      Avg accuracy: 0.926 Avg loss: 0.23974011365572612\n",
      "Validation Results - Epoch: 29      Avg accuracy: 0.9124579124579124 Avg loss: 0.2766258793848532\n",
      "Epoch[30] Loss: 0.18415139615535736\n",
      "Epoch[30] Loss: 0.32608768343925476\n",
      "Epoch[30] Loss: 0.17299789190292358\n",
      "Epoch[30] Loss: 0.3977612257003784\n",
      "Epoch[30] Loss: 0.4372807443141937\n",
      "Epoch[30] Loss: 0.12916645407676697\n",
      "Epoch[30] Loss: 0.2019624412059784\n",
      "Epoch[30] Loss: 0.08847165107727051\n",
      "Epoch[30] Loss: 0.23399920761585236\n",
      "Epoch[30] Loss: 0.3181821405887604\n",
      "Epoch[30] Loss: 0.2555117607116699\n",
      "Epoch[30] Loss: 0.40482664108276367\n",
      "Epoch[30] Loss: 0.2418948858976364\n",
      "Epoch[30] Loss: 0.5070178508758545\n",
      "Epoch[30] Loss: 0.14164887368679047\n",
      "Epoch[30] Loss: 0.20349296927452087\n",
      "Epoch[30] Loss: 0.15857505798339844\n",
      "Epoch[30] Loss: 0.1593075692653656\n",
      "Epoch[30] Loss: 0.3105294108390808\n",
      "Epoch[30] Loss: 0.2794345021247864\n",
      "Epoch[30] Loss: 0.4920714795589447\n",
      "Epoch[30] Loss: 0.18870244920253754\n",
      "Epoch[30] Loss: 0.41008472442626953\n",
      "Epoch[30] Loss: 0.17509448528289795\n",
      "Epoch[30] Loss: 0.13020232319831848\n",
      "Epoch[30] Loss: 0.22950583696365356\n",
      "Epoch[30] Loss: 0.3187122344970703\n",
      "Epoch[30] Loss: 0.39141225814819336\n",
      "Epoch[30] Loss: 0.09853219985961914\n",
      "Epoch[30] Loss: 0.24065953493118286\n",
      "Epoch[30] Loss: 0.3536028563976288\n",
      "Epoch[30] Loss: 0.2802967429161072\n",
      "Epoch[30] Loss: 0.35673263669013977\n",
      "Epoch[30] Loss: 0.13083575665950775\n",
      "Epoch[30] Loss: 0.15036308765411377\n",
      "Epoch[30] Loss: 0.3142378628253937\n",
      "Epoch[30] Loss: 0.4578211307525635\n",
      "Epoch[30] Loss: 0.20519885420799255\n",
      "Epoch[30] Loss: 0.17998628318309784\n",
      "Epoch[30] Loss: 0.22784581780433655\n",
      "Epoch[30] Loss: 0.18802562355995178\n",
      "Epoch[30] Loss: 0.24438868463039398\n",
      "Epoch[30] Loss: 0.24735328555107117\n",
      "Epoch[30] Loss: 0.37715715169906616\n",
      "Epoch[30] Loss: 0.08569289743900299\n",
      "Epoch[30] Loss: 0.3441073000431061\n",
      "Epoch[30] Loss: 0.2274528443813324\n",
      "Training Results - Epoch: 30      Avg accuracy: 0.9286666666666666 Avg loss: 0.23676977797349294\n",
      "Validation Results - Epoch: 30      Avg accuracy: 0.9057239057239057 Avg loss: 0.28043897023506037\n",
      "Epoch[31] Loss: 0.27874234318733215\n",
      "Epoch[31] Loss: 0.23324352502822876\n",
      "Epoch[31] Loss: 0.1931912750005722\n",
      "Epoch[31] Loss: 0.17345434427261353\n",
      "Epoch[31] Loss: 0.22661270201206207\n",
      "Epoch[31] Loss: 0.19574356079101562\n",
      "Epoch[31] Loss: 0.23559623956680298\n",
      "Epoch[31] Loss: 0.0882243663072586\n",
      "Epoch[31] Loss: 0.3721056878566742\n",
      "Epoch[31] Loss: 0.10525298118591309\n",
      "Epoch[31] Loss: 0.184962198138237\n",
      "Epoch[31] Loss: 0.24145810306072235\n",
      "Epoch[31] Loss: 0.2638341784477234\n",
      "Epoch[31] Loss: 0.25655728578567505\n",
      "Epoch[31] Loss: 0.2692745327949524\n",
      "Epoch[31] Loss: 0.20745189487934113\n",
      "Epoch[31] Loss: 0.6131006479263306\n",
      "Epoch[31] Loss: 0.2561571002006531\n",
      "Epoch[31] Loss: 0.37719738483428955\n",
      "Epoch[31] Loss: 0.14955542981624603\n",
      "Epoch[31] Loss: 0.25860899686813354\n",
      "Epoch[31] Loss: 0.18230940401554108\n",
      "Epoch[31] Loss: 0.05924956500530243\n",
      "Epoch[31] Loss: 0.2835146188735962\n",
      "Epoch[31] Loss: 0.17599162459373474\n",
      "Epoch[31] Loss: 0.36560261249542236\n",
      "Epoch[31] Loss: 0.2794916033744812\n",
      "Epoch[31] Loss: 0.3382318913936615\n",
      "Epoch[31] Loss: 0.5622736215591431\n",
      "Epoch[31] Loss: 0.10881416499614716\n",
      "Epoch[31] Loss: 0.2793864905834198\n",
      "Epoch[31] Loss: 0.3433418869972229\n",
      "Epoch[31] Loss: 0.21296870708465576\n",
      "Epoch[31] Loss: 0.2889922857284546\n",
      "Epoch[31] Loss: 0.2639305889606476\n",
      "Epoch[31] Loss: 0.2883654832839966\n",
      "Epoch[31] Loss: 0.22386030852794647\n",
      "Epoch[31] Loss: 0.540428102016449\n",
      "Epoch[31] Loss: 0.1411266028881073\n",
      "Epoch[31] Loss: 0.1791064739227295\n",
      "Epoch[31] Loss: 0.5114457607269287\n",
      "Epoch[31] Loss: 0.2874342203140259\n",
      "Epoch[31] Loss: 0.2852287292480469\n",
      "Epoch[31] Loss: 0.18282711505889893\n",
      "Epoch[31] Loss: 0.1379195749759674\n",
      "Epoch[31] Loss: 0.2976948916912079\n",
      "Epoch[31] Loss: 0.1258496195077896\n",
      "Training Results - Epoch: 31      Avg accuracy: 0.9246666666666666 Avg loss: 0.24546866846084595\n",
      "Validation Results - Epoch: 31      Avg accuracy: 0.8821548821548821 Avg loss: 0.30341672796994346\n",
      "Epoch[32] Loss: 0.38035011291503906\n",
      "Epoch[32] Loss: 0.2683248519897461\n",
      "Epoch[32] Loss: 0.19640490412712097\n",
      "Epoch[32] Loss: 0.172567218542099\n",
      "Epoch[32] Loss: 0.203974649310112\n",
      "Epoch[32] Loss: 0.2241622805595398\n",
      "Epoch[32] Loss: 0.12916208803653717\n",
      "Epoch[32] Loss: 0.274189293384552\n",
      "Epoch[32] Loss: 0.20796586573123932\n",
      "Epoch[32] Loss: 0.32379722595214844\n",
      "Epoch[32] Loss: 0.2801264822483063\n",
      "Epoch[32] Loss: 0.09775494039058685\n",
      "Epoch[32] Loss: 0.2573530375957489\n",
      "Epoch[32] Loss: 0.5024516582489014\n",
      "Epoch[32] Loss: 0.2078378051519394\n",
      "Epoch[32] Loss: 0.27287372946739197\n",
      "Epoch[32] Loss: 0.3433001935482025\n",
      "Epoch[32] Loss: 0.405361533164978\n",
      "Epoch[32] Loss: 0.4553386867046356\n",
      "Epoch[32] Loss: 0.34065067768096924\n",
      "Epoch[32] Loss: 0.22435349225997925\n",
      "Epoch[32] Loss: 0.3080458343029022\n",
      "Epoch[32] Loss: 0.14475753903388977\n",
      "Epoch[32] Loss: 0.17522723972797394\n",
      "Epoch[32] Loss: 0.2391061931848526\n",
      "Epoch[32] Loss: 0.22352799773216248\n",
      "Epoch[32] Loss: 0.28859782218933105\n",
      "Epoch[32] Loss: 0.2962597608566284\n",
      "Epoch[32] Loss: 0.24949872493743896\n",
      "Epoch[32] Loss: 0.37185415625572205\n",
      "Epoch[32] Loss: 0.09177154302597046\n",
      "Epoch[32] Loss: 0.5502724647521973\n",
      "Epoch[32] Loss: 0.38226351141929626\n",
      "Epoch[32] Loss: 0.40081730484962463\n",
      "Epoch[32] Loss: 0.11342593282461166\n",
      "Epoch[32] Loss: 0.3935455083847046\n",
      "Epoch[32] Loss: 0.2845161557197571\n",
      "Epoch[32] Loss: 0.5442144870758057\n",
      "Epoch[32] Loss: 0.08325783908367157\n",
      "Epoch[32] Loss: 0.19492210447788239\n",
      "Epoch[32] Loss: 0.14346317946910858\n",
      "Epoch[32] Loss: 0.2039298564195633\n",
      "Epoch[32] Loss: 0.3744667172431946\n",
      "Epoch[32] Loss: 0.13451389968395233\n",
      "Epoch[32] Loss: 0.48605138063430786\n",
      "Epoch[32] Loss: 0.19829469919204712\n",
      "Epoch[32] Loss: 0.19364871084690094\n",
      "Training Results - Epoch: 32      Avg accuracy: 0.9333333333333333 Avg loss: 0.23459956991672515\n",
      "Validation Results - Epoch: 32      Avg accuracy: 0.9023569023569024 Avg loss: 0.2857118677410614\n",
      "Epoch[33] Loss: 0.35817065834999084\n",
      "Epoch[33] Loss: 0.1236429512500763\n",
      "Epoch[33] Loss: 0.3078545331954956\n",
      "Epoch[33] Loss: 0.1790505349636078\n",
      "Epoch[33] Loss: 0.40933412313461304\n",
      "Epoch[33] Loss: 0.34697502851486206\n",
      "Epoch[33] Loss: 0.19405797123908997\n",
      "Epoch[33] Loss: 0.39127370715141296\n",
      "Epoch[33] Loss: 0.366054505109787\n",
      "Epoch[33] Loss: 0.12939000129699707\n",
      "Epoch[33] Loss: 0.2488713562488556\n",
      "Epoch[33] Loss: 0.14687176048755646\n",
      "Epoch[33] Loss: 0.243572399020195\n",
      "Epoch[33] Loss: 0.4553804099559784\n",
      "Epoch[33] Loss: 0.16632111370563507\n",
      "Epoch[33] Loss: 0.165794238448143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: .\n",
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-5b331a05d82f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, data, max_epochs, epoch_length, seed)\u001B[0m\n\u001B[0;32m    848\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    849\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 850\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_internal_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    851\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    852\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_setup_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    950\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_iter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_len\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    951\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Engine run is terminating due to exception: %s.\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 952\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    953\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    954\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_iter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataloader_len\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_handle_exception\u001B[1;34m(self, e)\u001B[0m\n\u001B[0;32m    714\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEXCEPTION_RAISED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    715\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 716\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    717\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_internal_run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    935\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_setup_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    936\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 937\u001B[1;33m                 \u001B[0mhours\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmins\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_run_once_on_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    938\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    939\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhours\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmins\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_run_once_on_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    703\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    704\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Current run is terminating due to exception: %s.\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 705\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    706\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    707\u001B[0m         \u001B[0mtime_taken\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_handle_exception\u001B[1;34m(self, e)\u001B[0m\n\u001B[0;32m    714\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEXCEPTION_RAISED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    715\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 716\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    717\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    718\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\engine.py\u001B[0m in \u001B[0;36m_run_once_on_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    686\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miteration\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    687\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mITERATION_STARTED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 688\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_process_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    689\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fire_event\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mITERATION_COMPLETED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    690\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\__init__.py\u001B[0m in \u001B[0;36m_update\u001B[1;34m(engine, batch)\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0moutput_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mEngine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_update\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\ignite\\engine\\__init__.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x, y, y_pred, loss)\u001B[0m\n\u001B[0;32m     17\u001B[0m                               \u001B[0mdevice\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m                               \u001B[0mprepare_batch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_prepare_batch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m                               output_transform=lambda x, y, y_pred, loss: loss.item()):\n\u001B[0m\u001B[0;32m     20\u001B[0m     \"\"\"\n\u001B[0;32m     21\u001B[0m     \u001B[0mFactory\u001B[0m \u001B[0mfunction\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcreating\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtrainer\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msupervised\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_data, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'resnet101': resnet101.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    },\n",
    "    f'{PATH}/resnet101{month_day_hour_minute}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34.class_to_idx = data['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}